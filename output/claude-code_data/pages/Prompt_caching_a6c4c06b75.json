{
  "title": "Prompt caching",
  "content": "Prompt caching is a powerful feature that optimizes your API usage by allowing resuming from specific prefixes in your prompts. This approach significantly reduces processing time and costs for repetitive tasks or prompts with consistent elements.\n\nHere's an example of how to implement prompt caching with the Messages API using a `cache_control` block:\n\n```bash Shell\ncurl https://api.anthropic.com/v1/messages \\\n  -H \"content-type: application/json\" \\\n  -H \"x-api-key: $ANTHROPIC_API_KEY\" \\\n  -H \"anthropic-version: 2023-06-01\" \\\n  -d '{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"system\": [\n      {\n        \"type\": \"text\",\n        \"text\": \"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\\n\"\n      },\n      {\n        \"type\": \"text\",\n        \"text\": \"<the entire contents of Pride and Prejudice>\",\n        \"cache_control\": {\"type\": \"ephemeral\"}\n      }\n    ],\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"Analyze the major themes in Pride and Prejudice.\"\n      }\n    ]\n  }'",
  "code_samples": [],
  "headings": [],
  "url": "llms-txt#prompt-caching",
  "links": []
}