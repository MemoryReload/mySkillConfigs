{
  "title": "[Analysis Title]",
  "content": "## Executive summary\n[Overview]\n\n## Key findings\n[Adapt sections based on what you discover]\n\n## Recommendations\n[Tailor to the specific context]\n`\n\nFor Skills where output quality depends on seeing examples, provide input/output pairs just like in regular prompting:\n\nfeat(auth): implement JWT-based authentication\n\nAdd login endpoint and token validation middleware\n\nfix(reports): correct date formatting in timezone conversion\n\nUse UTC timestamps consistently across report generation\n\nchore: update dependencies and refactor error handling\n\n- Upgrade lodash to 4.17.21\n- Standardize error response format across endpoints\n`\n\nExamples help Claude understand the desired style and level of detail more clearly than descriptions alone.\n\n### Conditional workflow pattern\n\nGuide Claude through decision points:\n\n<Tip>\nIf workflows become large or complicated with many steps, consider pushing them into separate files and tell Claude to read the appropriate file based on the task at hand.\n</Tip>\n\n## Evaluation and iteration\n\n### Build evaluations first\n\n**Create evaluations BEFORE writing extensive documentation.** This ensures your Skill solves real problems rather than documenting imagined ones.\n\n**Evaluation-driven development:**\n1. **Identify gaps**: Run Claude on representative tasks without a Skill. Document specific failures or missing context\n2. **Create evaluations**: Build three scenarios that test these gaps\n3. **Establish baseline**: Measure Claude's performance without the Skill\n4. **Write minimal instructions**: Create just enough content to address the gaps and pass evaluations\n5. **Iterate**: Execute evaluations, compare against baseline, and refine\n\nThis approach ensures you're solving actual problems rather than anticipating requirements that may never materialize.\n\n**Evaluation structure**:\n\n<Note>\nThis example demonstrates a data-driven evaluation with a simple testing rubric. We do not currently provide a built-in way to run these evaluations. Users can create their own evaluation system. Evaluations are your source of truth for measuring Skill effectiveness.\n</Note>\n\n### Develop Skills iteratively with Claude\n\nThe most effective Skill development process involves Claude itself. Work with one instance of Claude (\"Claude A\") to create a Skill that will be used by other instances (\"Claude B\"). Claude A helps you design and refine instructions, while Claude B tests them in real tasks. This works because Claude models understand both how to write effective agent instructions and what information agents need.\n\n**Creating a new Skill:**\n\n1. **Complete a task without a Skill**: Work through a problem with Claude A using normal prompting. As you work, you'll naturally provide context, explain preferences, and share procedural knowledge. Notice what information you repeatedly provide.\n\n2. **Identify the reusable pattern**: After completing the task, identify what context you provided that would be useful for similar future tasks.\n\n**Example**: If you worked through a BigQuery analysis, you might have provided table names, field definitions, filtering rules (like \"always exclude test accounts\"), and common query patterns.\n\n3. **Ask Claude A to create a Skill**: \"Create a Skill that captures this BigQuery analysis pattern we just used. Include the table schemas, naming conventions, and the rule about filtering test accounts.\"\n\n<Tip>\n   Claude models understand the Skill format and structure natively. You don't need special system prompts or a \"writing skills\" skill to get Claude to help create Skills. Simply ask Claude to create a Skill and it will generate properly structured SKILL.md content with appropriate frontmatter and body content.\n   </Tip>\n\n4. **Review for conciseness**: Check that Claude A hasn't added unnecessary explanations. Ask: \"Remove the explanation about what win rate means - Claude already knows that.\"\n\n5. **Improve information architecture**: Ask Claude A to organize the content more effectively. For example: \"Organize this so the table schema is in a separate reference file. We might add more tables later.\"\n\n6. **Test on similar tasks**: Use the Skill with Claude B (a fresh instance with the Skill loaded) on related use cases. Observe whether Claude B finds the right information, applies rules correctly, and handles the task successfully.\n\n7. **Iterate based on observation**: If Claude B struggles or misses something, return to Claude A with specifics: \"When Claude used this Skill, it forgot to filter by date for Q4. Should we add a section about date filtering patterns?\"\n\n**Iterating on existing Skills:**\n\nThe same hierarchical pattern continues when improving Skills. You alternate between:\n- **Working with Claude A** (the expert who helps refine the Skill)\n- **Testing with Claude B** (the agent using the Skill to perform real work)\n- **Observing Claude B's behavior** and bringing insights back to Claude A\n\n1. **Use the Skill in real workflows**: Give Claude B (with the Skill loaded) actual tasks, not test scenarios\n\n2. **Observe Claude B's behavior**: Note where it struggles, succeeds, or makes unexpected choices\n\n**Example observation**: \"When I asked Claude B for a regional sales report, it wrote the query but forgot to filter out test accounts, even though the Skill mentions this rule.\"\n\n3. **Return to Claude A for improvements**: Share the current SKILL.md and describe what you observed. Ask: \"I noticed Claude B forgot to filter test accounts when I asked for a regional report. The Skill mentions filtering, but maybe it's not prominent enough?\"\n\n4. **Review Claude A's suggestions**: Claude A might suggest reorganizing to make rules more prominent, using stronger language like \"MUST filter\" instead of \"always filter\", or restructuring the workflow section.\n\n5. **Apply and test changes**: Update the Skill with Claude A's refinements, then test again with Claude B on similar requests\n\n6. **Repeat based on usage**: Continue this observe-refine-test cycle as you encounter new scenarios. Each iteration improves the Skill based on real agent behavior, not assumptions.\n\n**Gathering team feedback:**\n\n1. Share Skills with teammates and observe their usage\n2. Ask: Does the Skill activate when expected? Are instructions clear? What's missing?\n3. Incorporate feedback to address blind spots in your own usage patterns\n\n**Why this approach works**: Claude A understands agent needs, you provide domain expertise, Claude B reveals gaps through real usage, and iterative refinement improves Skills based on observed behavior rather than assumptions.\n\n### Observe how Claude navigates Skills\n\nAs you iterate on Skills, pay attention to how Claude actually uses them in practice. Watch for:\n\n- **Unexpected exploration paths**: Does Claude read files in an order you didn't anticipate? This might indicate your structure isn't as intuitive as you thought\n- **Missed connections**: Does Claude fail to follow references to important files? Your links might need to be more explicit or prominent\n- **Overreliance on certain sections**: If Claude repeatedly reads the same file, consider whether that content should be in the main SKILL.md instead\n- **Ignored content**: If Claude never accesses a bundled file, it might be unnecessary or poorly signaled in the main instructions\n\nIterate based on these observations rather than assumptions. The 'name' and 'description' in your Skill's metadata are particularly critical. Claude uses these when deciding whether to trigger the Skill in response to the current task. Make sure they clearly describe what the Skill does and when it should be used.\n\n## Anti-patterns to avoid\n\n### Avoid Windows-style paths\n\nAlways use forward slashes in file paths, even on Windows:\n\n- ✓ **Good**: `scripts/helper.py`, `reference/guide.md`\n- ✗ **Avoid**: `scripts\\helper.py`, `reference\\guide.md`\n\nUnix-style paths work across all platforms, while Windows-style paths cause errors on Unix systems.\n\n### Avoid offering too many options\n\nDon't present multiple approaches unless necessary:\n\npython\nimport pdfplumber\n`\n\n## Advanced: Skills with executable code\n\nThe sections below focus on Skills that include executable scripts. If your Skill uses only markdown instructions, skip to [Checklist for effective Skills](#checklist-for-effective-skills).\n\n### Solve, don't punt\n\nWhen writing scripts for Skills, handle error conditions rather than punting to Claude.\n\n**Good example: Handle errors explicitly**:\n\n**Bad example: Punt to Claude**:\n\nConfiguration parameters should also be justified and documented to avoid \"voodoo constants\" (Ousterhout's law). If you don't know the right value, how will Claude determine it?\n\n**Good example: Self-documenting**:\n```python",
  "code_samples": [
    {
      "code": "Adjust sections as needed for the specific analysis type.",
      "language": "unknown"
    },
    {
      "code": "## Commit message format\n\nGenerate commit messages following these examples:\n\n**Example 1:**\nInput: Added user authentication with JWT tokens\nOutput:",
      "language": "markdown"
    },
    {
      "code": "**Example 2:**\nInput: Fixed bug where dates displayed incorrectly in reports\nOutput:",
      "language": "unknown"
    },
    {
      "code": "**Example 3:**\nInput: Updated dependencies and refactored error handling\nOutput:",
      "language": "unknown"
    },
    {
      "code": "Follow this style: type(scope): brief description, then detailed explanation.",
      "language": "unknown"
    },
    {
      "code": "## Document modification workflow\n\n1. Determine the modification type:\n\n   **Creating new content?** → Follow \"Creation workflow\" below\n   **Editing existing content?** → Follow \"Editing workflow\" below\n\n2. Creation workflow:\n   - Use docx-js library\n   - Build document from scratch\n   - Export to .docx format\n\n3. Editing workflow:\n   - Unpack existing document\n   - Modify XML directly\n   - Validate after each change\n   - Repack when complete",
      "language": "markdown"
    },
    {
      "code": "{\n  \"skills\": [\"pdf-processing\"],\n  \"query\": \"Extract all text from this PDF file and save it to output.txt\",\n  \"files\": [\"test-files/document.pdf\"],\n  \"expected_behavior\": [\n    \"Successfully reads the PDF file using an appropriate PDF processing library or command-line tool\",\n    \"Extracts text content from all pages in the document without missing any pages\",\n    \"Saves the extracted text to a file named output.txt in a clear, readable format\"\n  ]\n}",
      "language": "json"
    },
    {
      "code": "**Bad example: Too many choices** (confusing):\n\"You can use pypdf, or pdfplumber, or PyMuPDF, or pdf2image, or...\"\n\n**Good example: Provide a default** (with escape hatch):\n\"Use pdfplumber for text extraction:",
      "language": "markdown"
    },
    {
      "code": "For scanned PDFs requiring OCR, use pdf2image with pytesseract instead.\"",
      "language": "unknown"
    },
    {
      "code": "def process_file(path):\n    \"\"\"Process a file, creating it if it doesn't exist.\"\"\"\n    try:\n        with open(path) as f:\n            return f.read()\n    except FileNotFoundError:\n        # Create file with default content instead of failing\n        print(f\"File {path} not found, creating default\")\n        with open(path, 'w') as f:\n            f.write('')\n        return ''\n    except PermissionError:\n        # Provide alternative instead of failing\n        print(f\"Cannot access {path}, using default\")\n        return ''",
      "language": "python"
    },
    {
      "code": "def process_file(path):\n    # Just fail and let Claude figure it out\n    return open(path).read()",
      "language": "python"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Executive summary",
      "id": "executive-summary"
    },
    {
      "level": "h2",
      "text": "Key findings",
      "id": "key-findings"
    },
    {
      "level": "h2",
      "text": "Recommendations",
      "id": "recommendations"
    },
    {
      "level": "h3",
      "text": "Examples pattern",
      "id": "examples-pattern"
    },
    {
      "level": "h2",
      "text": "Commit message format",
      "id": "commit-message-format"
    },
    {
      "level": "h3",
      "text": "Conditional workflow pattern",
      "id": "conditional-workflow-pattern"
    },
    {
      "level": "h2",
      "text": "Document modification workflow",
      "id": "document-modification-workflow"
    },
    {
      "level": "h2",
      "text": "Evaluation and iteration",
      "id": "evaluation-and-iteration"
    },
    {
      "level": "h3",
      "text": "Build evaluations first",
      "id": "build-evaluations-first"
    },
    {
      "level": "h3",
      "text": "Develop Skills iteratively with Claude",
      "id": "develop-skills-iteratively-with-claude"
    },
    {
      "level": "h3",
      "text": "Observe how Claude navigates Skills",
      "id": "observe-how-claude-navigates-skills"
    },
    {
      "level": "h2",
      "text": "Anti-patterns to avoid",
      "id": "anti-patterns-to-avoid"
    },
    {
      "level": "h3",
      "text": "Avoid Windows-style paths",
      "id": "avoid-windows-style-paths"
    },
    {
      "level": "h3",
      "text": "Avoid offering too many options",
      "id": "avoid-offering-too-many-options"
    },
    {
      "level": "h2",
      "text": "Advanced: Skills with executable code",
      "id": "advanced:-skills-with-executable-code"
    },
    {
      "level": "h3",
      "text": "Solve, don't punt",
      "id": "solve,-don't-punt"
    }
  ],
  "url": "llms-txt#[analysis-title]",
  "links": []
}