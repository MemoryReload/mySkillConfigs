{
  "title": "Create a Text Completion (Go)",
  "content": "URL: https://platform.claude.com/docs/en/api/go/completions/create\n\n`client.Completions.New(ctx, params) (*Completion, error)`\n\n**post** `/v1/complete`\n\n[Legacy] Create a Text Completion.\n\nThe Text Completions API is a legacy API. We recommend using the [Messages API](https://docs.claude.com/en/api/messages) going forward.\n\nFuture models and features will not be compatible with Text Completions. See our [migration guide](https://docs.claude.com/en/api/migrating-from-text-completions-to-messages) for guidance in migrating from Text Completions to Messages.\n\n- `params CompletionNewParams`\n\n- `MaxTokensToSample param.Field[int64]`\n\nBody param: The maximum number of tokens to generate before stopping.\n\nNote that our models may stop _before_ reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.\n\n- `Model param.Field[Model]`\n\nBody param: The model that will complete your prompt.\n\nSee [models](https://docs.anthropic.com/en/docs/models-overview) for additional details and options.\n\n- `Prompt param.Field[string]`\n\nBody param: The prompt that you want Claude to complete.\n\nFor proper response generation you will need to format your prompt using alternating `\n\nAssistant:` conversational turns. For example:\n\nSee [prompt validation](https://docs.claude.com/en/api/prompt-validation) and our guide to [prompt design](https://docs.claude.com/en/docs/intro-to-prompting) for more details.\n\n- `Metadata param.Field[Metadata]`\n\nBody param: An object describing metadata about the request.\n\n- `StopSequences param.Field[[]string]`\n\nBody param: Sequences that will cause the model to stop generating.\n\nOur models stop on `\"\n\nHuman:\"`, and may include additional built-in stop sequences in the future. By providing the stop_sequences parameter, you may include additional strings that will cause the model to stop generating.\n\n- `Temperature param.Field[float64]`\n\nBody param: Amount of randomness injected into the response.\n\nDefaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to `1.0` for creative and generative tasks.\n\nNote that even with `temperature` of `0.0`, the results will not be fully deterministic.\n\n- `TopK param.Field[int64]`\n\nBody param: Only sample from the top K options for each subsequent token.\n\nUsed to remove \"long tail\" low probability responses. [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).\n\nRecommended for advanced use cases only. You usually only need to use `temperature`.\n\n- `TopP param.Field[float64]`\n\nBody param: Use nucleus sampling.\n\nIn nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by `top_p`. You should either alter `temperature` or `top_p`, but not both.\n\nRecommended for advanced use cases only. You usually only need to use `temperature`.\n\n- `Betas param.Field[[]AnthropicBeta]`\n\nHeader param: Optional header to specify the beta version(s) you want to use.\n\n- `type AnthropicBeta string`\n\n- `const AnthropicBetaMessageBatches2024_09_24 AnthropicBeta = \"message-batches-2024-09-24\"`\n\n- `const AnthropicBetaPromptCaching2024_07_31 AnthropicBeta = \"prompt-caching-2024-07-31\"`\n\n- `const AnthropicBetaComputerUse2024_10_22 AnthropicBeta = \"computer-use-2024-10-22\"`\n\n- `const AnthropicBetaComputerUse2025_01_24 AnthropicBeta = \"computer-use-2025-01-24\"`\n\n- `const AnthropicBetaPDFs2024_09_25 AnthropicBeta = \"pdfs-2024-09-25\"`\n\n- `const AnthropicBetaTokenCounting2024_11_01 AnthropicBeta = \"token-counting-2024-11-01\"`\n\n- `const AnthropicBetaTokenEfficientTools2025_02_19 AnthropicBeta = \"token-efficient-tools-2025-02-19\"`\n\n- `const AnthropicBetaOutput128k2025_02_19 AnthropicBeta = \"output-128k-2025-02-19\"`\n\n- `const AnthropicBetaFilesAPI2025_04_14 AnthropicBeta = \"files-api-2025-04-14\"`\n\n- `const AnthropicBetaMCPClient2025_04_04 AnthropicBeta = \"mcp-client-2025-04-04\"`\n\n- `const AnthropicBetaMCPClient2025_11_20 AnthropicBeta = \"mcp-client-2025-11-20\"`\n\n- `const AnthropicBetaDevFullThinking2025_05_14 AnthropicBeta = \"dev-full-thinking-2025-05-14\"`\n\n- `const AnthropicBetaInterleavedThinking2025_05_14 AnthropicBeta = \"interleaved-thinking-2025-05-14\"`\n\n- `const AnthropicBetaCodeExecution2025_05_22 AnthropicBeta = \"code-execution-2025-05-22\"`\n\n- `const AnthropicBetaExtendedCacheTTL2025_04_11 AnthropicBeta = \"extended-cache-ttl-2025-04-11\"`\n\n- `const AnthropicBetaContext1m2025_08_07 AnthropicBeta = \"context-1m-2025-08-07\"`\n\n- `const AnthropicBetaContextManagement2025_06_27 AnthropicBeta = \"context-management-2025-06-27\"`\n\n- `const AnthropicBetaModelContextWindowExceeded2025_08_26 AnthropicBeta = \"model-context-window-exceeded-2025-08-26\"`\n\n- `const AnthropicBetaSkills2025_10_02 AnthropicBeta = \"skills-2025-10-02\"`\n\n- `type Completion struct{…}`\n\nUnique object identifier.\n\nThe format and length of IDs may change over time.\n\n- `Completion string`\n\nThe resulting completion up to and excluding the stop sequences.\n\nThe model that will complete your prompt.\n\nSee [models](https://docs.anthropic.com/en/docs/models-overview) for additional details and options.\n\n- `type Model string`\n\nThe model that will complete your prompt.\n\nSee [models](https://docs.anthropic.com/en/docs/models-overview) for additional details and options.\n\n- `const ModelClaudeOpus4_5_20251101 Model = \"claude-opus-4-5-20251101\"`\n\nPremium model combining maximum intelligence with practical performance\n\n- `const ModelClaudeOpus4_5 Model = \"claude-opus-4-5\"`\n\nPremium model combining maximum intelligence with practical performance\n\n- `const ModelClaude3_7SonnetLatest Model = \"claude-3-7-sonnet-latest\"`\n\nHigh-performance model with early extended thinking\n\n- `const ModelClaude3_7Sonnet20250219 Model = \"claude-3-7-sonnet-20250219\"`\n\nHigh-performance model with early extended thinking\n\n- `const ModelClaude3_5HaikuLatest Model = \"claude-3-5-haiku-latest\"`\n\nFastest and most compact model for near-instant responsiveness\n\n- `const ModelClaude3_5Haiku20241022 Model = \"claude-3-5-haiku-20241022\"`\n\n- `const ModelClaudeHaiku4_5 Model = \"claude-haiku-4-5\"`\n\nHybrid model, capable of near-instant responses and extended thinking\n\n- `const ModelClaudeHaiku4_5_20251001 Model = \"claude-haiku-4-5-20251001\"`\n\nHybrid model, capable of near-instant responses and extended thinking\n\n- `const ModelClaudeSonnet4_20250514 Model = \"claude-sonnet-4-20250514\"`\n\nHigh-performance model with extended thinking\n\n- `const ModelClaudeSonnet4_0 Model = \"claude-sonnet-4-0\"`\n\nHigh-performance model with extended thinking\n\n- `const ModelClaude4Sonnet20250514 Model = \"claude-4-sonnet-20250514\"`\n\nHigh-performance model with extended thinking\n\n- `const ModelClaudeSonnet4_5 Model = \"claude-sonnet-4-5\"`\n\nOur best model for real-world agents and coding\n\n- `const ModelClaudeSonnet4_5_20250929 Model = \"claude-sonnet-4-5-20250929\"`\n\nOur best model for real-world agents and coding\n\n- `const ModelClaudeOpus4_0 Model = \"claude-opus-4-0\"`\n\nOur most capable model\n\n- `const ModelClaudeOpus4_20250514 Model = \"claude-opus-4-20250514\"`\n\nOur most capable model\n\n- `const ModelClaude4Opus20250514 Model = \"claude-4-opus-20250514\"`\n\nOur most capable model\n\n- `const ModelClaudeOpus4_1_20250805 Model = \"claude-opus-4-1-20250805\"`\n\nOur most capable model\n\n- `const ModelClaude3OpusLatest Model = \"claude-3-opus-latest\"`\n\nExcels at writing and complex tasks\n\n- `const ModelClaude_3_Opus_20240229 Model = \"claude-3-opus-20240229\"`\n\nExcels at writing and complex tasks\n\n- `const ModelClaude_3_Haiku_20240307 Model = \"claude-3-haiku-20240307\"`\n\nOur previous most fast and cost-effective\n\n- `StopReason string`\n\nThe reason that we stopped.\n\nThis may be one the following values:\n\n* `\"stop_sequence\"`: we reached a stop sequence — either provided by you via the `stop_sequences` parameter, or a stop sequence built into the model\n    * `\"max_tokens\"`: we exceeded `max_tokens_to_sample` or the model's maximum\n\nFor Text Completions, this is always `\"completion\"`.\n\n- `const CompletionCompletion Completion = \"completion\"`",
  "code_samples": [
    {
      "code": "\"\n    \n    Human: {userQuestion}\n    \n    Assistant:\"",
      "language": "unknown"
    },
    {
      "code": "package main\n\nimport (\n  \"context\"\n  \"fmt\"\n\n  \"github.com/anthropics/anthropic-sdk-go\"\n  \"github.com/anthropics/anthropic-sdk-go/option\"\n)\n\nfunc main() {\n  client := anthropic.NewClient(\n    option.WithAPIKey(\"my-anthropic-api-key\"),\n  )\n  completion, err := client.Completions.New(context.TODO(), anthropic.CompletionNewParams{\n    MaxTokensToSample: 256,\n    Model: anthropic.ModelClaudeOpus4_5_20251101,\n    Prompt: \"\\n\\nHuman: Hello, world!\\n\\nAssistant:\",\n  })\n  if err != nil {\n    panic(err.Error())\n  }\n  fmt.Printf(\"%+v\\n\", completion.ID)\n}",
      "language": "go"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Create",
      "id": "create"
    },
    {
      "level": "h3",
      "text": "Parameters",
      "id": "parameters"
    },
    {
      "level": "h3",
      "text": "Returns",
      "id": "returns"
    },
    {
      "level": "h3",
      "text": "Example",
      "id": "example"
    }
  ],
  "url": "llms-txt#create-a-text-completion-(go)",
  "links": []
}