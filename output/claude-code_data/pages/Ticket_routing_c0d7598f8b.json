{
  "title": "Ticket routing",
  "content": "This guide walks through how to harness Claude's advanced natural language understanding capabilities to classify customer support tickets at scale based on customer intent, urgency, prioritization, customer profile, and more.\n\n## Define whether to use Claude for ticket routing\n\nHere are some key indicators that you should use an LLM like Claude  instead of traditional ML approaches for your classification task:\n\n<section title=\"You have limited labeled training data available\">\n\nTraditional ML processes require massive labeled datasets. Claude's pre-trained model can effectively classify tickets with just a few dozen labeled examples, significantly reducing data preparation time and costs.\n    \n</section>\n    <section title=\"Your classification categories are likely to change or evolve over time\">\n\nOnce a traditional ML approach has been established, changing it is a laborious and data-intensive undertaking. On the other hand, as your product or customer needs evolve, Claude can easily adapt to changes in class definitions or new classes without extensive relabeling of training data.\n    \n</section>\n    <section title=\"You need to handle complex, unstructured text inputs\">\n\nTraditional ML models often struggle with unstructured data and require extensive feature engineering. Claude's advanced language understanding allows for accurate classification based on content and context, rather than relying on strict ontological structures.\n    \n</section>\n    <section title=\"Your classification rules are based on semantic understanding\">\n\nTraditional ML approaches often rely on bag-of-words models or simple pattern matching. Claude excels at understanding and applying underlying rules when classes are defined by conditions rather than examples.\n    \n</section>\n    <section title=\"You require interpretable reasoning for classification decisions\">\n\nMany traditional ML models provide little insight into their decision-making process. Claude can provide human-readable explanations for its classification decisions, building trust in the automation system and facilitating easy adaptation if needed.\n    \n</section>\n    <section title=\"You want to handle edge cases and ambiguous tickets more effectively\">\n\nTraditional ML systems often struggle with outliers and ambiguous inputs, frequently misclassifying them or defaulting to a catch-all category. Claude's natural language processing capabilities allow it to better interpret context and nuance in support tickets, potentially reducing the number of misrouted or unclassified tickets that require manual intervention.\n    \n</section>\n    <section title=\"You need multilingual support without maintaining separate models\">\n\nTraditional ML approaches typically require separate models or extensive translation processes for each supported language. Claude's multilingual capabilities allow it to classify tickets in various languages without the need for separate models or extensive translation processes, streamlining support for global customer bases.\n    \n</section>\n\n##  Build and deploy your LLM support workflow\n\n### Understand your current support approach\nBefore diving into automation, it's crucial to understand your existing ticketing system. Start by investigating how your support team currently handles ticket routing.\n\nConsider questions like:\n* What criteria are used to determine what SLA/service offering is applied?\n* Is ticket routing used to determine which tier of support or product specialist a ticket goes to?\n* Are there any automated rules or workflows already in place? In what cases do they fail? \n* How are edge cases or ambiguous tickets handled?\n* How does the team prioritize tickets?\n\nThe more you know about how humans handle certain cases, the better you will be able to work with Claude to do the task.\n\n### Define user intent categories\nA well-defined list of user intent categories is crucial for accurate support ticket classification with Claude. Claude’s ability to route tickets effectively within your system is directly proportional to how well-defined your system’s categories are.\n\nHere are some example user intent categories and subcategories.\n\n<section title=\"Technical issue\">\n\n* Hardware problem\n        * Software bug\n        * Compatibility issue\n        * Performance problem\n    \n</section>\n    <section title=\"Account management\">\n\n* Password reset\n        * Account access issues\n        * Billing inquiries\n        * Subscription changes\n    \n</section>\n    <section title=\"Product information\">\n\n* Feature inquiries\n        * Product compatibility questions\n        * Pricing information\n        * Availability inquiries\n    \n</section>\n    <section title=\"User guidance\">\n\n* How-to questions\n        * Feature usage assistance\n        * Best practices advice\n        * Troubleshooting guidance\n    \n</section>\n    <section title=\"Feedback\">\n\n* Bug reports\n        * Feature requests\n        * General feedback or suggestions\n        * Complaints\n    \n</section>\n    <section title=\"Order-related\">\n\n* Order status inquiries\n        * Shipping information\n        * Returns and exchanges\n        * Order modifications\n    \n</section>\n    <section title=\"Service request\">\n\n* Installation assistance\n        * Upgrade requests\n        * Maintenance scheduling\n        * Service cancellation\n    \n</section>\n    <section title=\"Security concerns\">\n\n* Data privacy inquiries\n        * Suspicious activity reports\n        * Security feature assistance\n    \n</section>\n    <section title=\"Compliance and legal\">\n\n* Regulatory compliance questions\n        * Terms of service inquiries\n        * Legal documentation requests\n    \n</section>\n    <section title=\"Emergency support\">\n\n* Critical system failures\n        * Urgent security issues\n        * Time-sensitive problems\n    \n</section>\n    <section title=\"Training and education\">\n\n* Product training requests\n        * Documentation inquiries\n        * Webinar or workshop information\n    \n</section>\n    <section title=\"Integration and API\">\n\n* Integration assistance\n        * API usage questions\n        * Third-party compatibility inquiries\n    \n</section>\n\nIn addition to intent, ticket routing and prioritization may also be influenced by other factors such as urgency, customer type, SLAs, or language. Be sure to consider other routing criteria when building your automated routing system.\n\n### Establish success criteria\n\nWork with your support team to [define clear success criteria](/docs/en/test-and-evaluate/define-success) with measurable benchmarks, thresholds, and goals.\n\nHere are some standard criteria and benchmarks when using LLMs for support ticket routing:\n\n<section title=\"Classification consistency\">\n\nThis metric assesses how consistently Claude classifies similar tickets over time. It's crucial for maintaining routing reliability. Measure this by periodically testing the model with a set of standardized inputs and aiming for a consistency rate of 95% or higher.\n    \n</section>\n    <section title=\"Adaptation speed\">\n\nThis measures how quickly Claude can adapt to new categories or changing ticket patterns. Test this by introducing new ticket types and measuring the time it takes for the model to achieve satisfactory accuracy (e.g., >90%) on these new categories. Aim for adaptation within 50-100 sample tickets.\n    \n</section>\n    <section title=\"Multilingual handling\">\n\nThis assesses Claude's ability to accurately route tickets in multiple languages. Measure the routing accuracy across different languages, aiming for no more than a 5-10% drop in accuracy for non-primary languages.\n    \n</section>\n    <section title=\"Edge case handling\">\n\nThis evaluates Claude's performance on unusual or complex tickets. Create a test set of edge cases and measure the routing accuracy, aiming for at least 80% accuracy on these challenging inputs.\n    \n</section>\n    <section title=\"Bias mitigation\">\n\nThis measures Claude's fairness in routing across different customer demographics. Regularly audit routing decisions for potential biases, aiming for consistent routing accuracy (within 2-3%) across all customer groups.\n    \n</section>\n    <section title=\"Prompt efficiency\">\n\nIn situations where minimizing token count is crucial, this criteria assesses how well Claude performs with minimal context. Measure routing accuracy with varying amounts of context provided, aiming for 90%+ accuracy with just the ticket title and a brief description.\n    \n</section>\n    <section title=\"Explainability score\">\n\nThis evaluates the quality and relevance of Claude's explanations for its routing decisions. Human raters can score explanations on a scale (e.g., 1-5), with the goal of achieving an average score of 4 or higher.\n    \n</section>\n\nHere are some common success criteria that may be useful regardless of whether an LLM is used:\n\n<section title=\"Routing accuracy\">\n\nRouting accuracy measures how often tickets are correctly assigned to the appropriate team or individual on the first try. This is typically measured as a percentage of correctly routed tickets out of total tickets. Industry benchmarks often aim for 90-95% accuracy, though this can vary based on the complexity of the support structure.\n    \n</section>\n    <section title=\"Time-to-assignment\">\n\nThis metric tracks how quickly tickets are assigned after being submitted. Faster assignment times generally lead to quicker resolutions and improved customer satisfaction. Best-in-class systems often achieve average assignment times of under 5 minutes, with many aiming for near-instantaneous routing (which is possible with LLM implementations). \n    \n</section>\n    <section title=\"Rerouting rate\">\n\nThe rerouting rate indicates how often tickets need to be reassigned after initial routing. A lower rate suggests more accurate initial routing. Aim for a rerouting rate below 10%, with top-performing systems achieving rates as low as 5% or less.\n    \n</section>\n    <section title=\"First-contact resolution rate\">\n\nThis measures the percentage of tickets resolved during the first interaction with the customer. Higher rates indicate efficient routing and well-prepared support teams. Industry benchmarks typically range from 70-75%, with top performers achieving rates of 80% or higher.\n    \n</section>\n    <section title=\"Average handling time\">\n\nAverage handling time measures how long it takes to resolve a ticket from start to finish. Efficient routing can significantly reduce this time. Benchmarks vary widely by industry and complexity, but many organizations aim to keep average handling time under 24 hours for non-critical issues.\n    \n</section>\n    <section title=\"Customer satisfaction scores\">\n\nOften measured through post-interaction surveys, these scores reflect overall customer happiness with the support process. Effective routing contributes to higher satisfaction. Aim for CSAT scores of 90% or higher, with top performers often achieving 95%+ satisfaction rates.\n    \n</section>\n    <section title=\"Escalation rate\">\n\nThis measures how often tickets need to be escalated to higher tiers of support. Lower escalation rates often indicate more accurate initial routing. Strive for an escalation rate below 20%, with best-in-class systems achieving rates of 10% or less.\n    \n</section>\n    <section title=\"Agent productivity\">\n\nThis metric looks at how many tickets agents can handle effectively after implementing the routing solution. Improved routing should increase productivity. Measure this by tracking tickets resolved per agent per day or hour, aiming for a 10-20% improvement after implementing a new routing system.\n    \n</section>\n    <section title=\"Self-service deflection rate\">\n\nThis measures the percentage of potential tickets resolved through self-service options before entering the routing system. Higher rates indicate effective pre-routing triage. Aim for a deflection rate of 20-30%, with top performers achieving rates of 40% or higher.\n    \n</section>\n    <section title=\"Cost per ticket\">\n\nThis metric calculates the average cost to resolve each support ticket. Efficient routing should help reduce this cost over time. While benchmarks vary widely, many organizations aim to reduce cost per ticket by 10-15% after implementing an improved routing system.\n    \n</section>\n\n### Choose the right Claude model\n\nThe choice of model depends on the trade-offs between cost, accuracy, and response time.\n\nMany customers have found `claude-haiku-4-5-20251001` an ideal model for ticket routing, as it is the fastest and most cost-effective model in the Claude 4 family while still delivering excellent results. If your classification problem requires deep subject matter expertise or a large volume of intent categories complex reasoning, you may opt for the [larger Sonnet model](/docs/en/about-claude/models).\n\n### Build a strong prompt\n\nTicket routing is a type of classification task. Claude analyzes the content of a support ticket and classifies it into predefined categories based on the issue type, urgency, required expertise, or other relevant factors.\n\nLet’s write a ticket classification prompt. Our initial prompt should contain the contents of the user request and return both the reasoning and the intent.\n\n<Tip>\nTry the [prompt generator](/docs/en/prompt-generator) on the [Claude Console](/login) to have Claude write a first draft for you.\n</Tip>\n\nHere's an example ticket routing classification prompt:\n\nLet's break down the key components of this prompt:\n* We use Python f-strings to create the prompt template, allowing the `ticket_contents` to be inserted into the `<request>` tags.\n* We give  Claude a clearly defined role as a classification system that carefully analyzes the ticket content to determine the customer's core intent and needs.\n* We instruct Claude on proper output formatting, in this case to provide its reasoning and analysis inside `<reasoning>` tags, followed by the appropriate classification label inside `<intent>` tags.\n* We specify the valid intent categories: \"Support, Feedback, Complaint\", \"Order Tracking\", and \"Refund/Exchange\".\n* We include a few examples (a.k.a. few-shot prompting) to illustrate how the output should be formatted, which improves accuracy and consistency.\n\nThe reason we want to have Claude split its response into various XML tag sections is so that we can use regular expressions to separately extract the reasoning and intent from the output. This allows us to create targeted next steps in the ticket routing workflow, such as using only the intent to decide which person to route the ticket to.\n\n### Deploy your prompt\n\nIt’s hard to know how well your prompt works without deploying it in a test production setting and [running evaluations](/docs/en/test-and-evaluate/develop-tests).\n\nLet’s build the deployment structure. Start by defining the method signature for wrapping our call to Claude. We'll take the method we’ve already begun to write, which has `ticket_contents` as input, and now return a tuple of `reasoning` and `intent` as output. If you have an existing automation using traditional ML, you'll want to follow that method signature instead.\n\n```python\nimport anthropic\nimport re",
  "code_samples": [
    {
      "code": "def classify_support_request(ticket_contents):\n    # Define the prompt for the classification task\n    classification_prompt = f\"\"\"You will be acting as a customer support ticket classification system. Your task is to analyze customer support requests and output the appropriate classification intent for each request, along with your reasoning. \n\n        Here is the customer support request you need to classify:\n\n        <request>{ticket_contents}</request>\n\n        Please carefully analyze the above request to determine the customer's core intent and needs. Consider what the customer is asking for has concerns about.\n\n        First, write out your reasoning and analysis of how to classify this request inside <reasoning> tags.\n\n        Then, output the appropriate classification label for the request inside a <intent> tag. The valid intents are:\n        <intents>\n        <intent>Support, Feedback, Complaint</intent>\n        <intent>Order Tracking</intent>\n        <intent>Refund/Exchange</intent>\n        </intents>\n\n        A request may have ONLY ONE applicable intent. Only include the intent that is most applicable to the request.\n\n        As an example, consider the following request:\n        <request>Hello! I had high-speed fiber internet installed on Saturday and my installer, Kevin, was absolutely fantastic! Where can I send my positive review? Thanks for your help!</request>\n\n        Here is an example of how your output should be formatted (for the above example request):\n        <reasoning>The user seeks information in order to leave positive feedback.</reasoning>\n        <intent>Support, Feedback, Complaint</intent>\n\n        Here are a few more examples:\n        <examples>\n        <example 2>\n        Example 2 Input:\n        <request>I wanted to write and personally thank you for the compassion you showed towards my family during my father's funeral this past weekend. Your staff was so considerate and helpful throughout this whole process; it really took a load off our shoulders. The visitation brochures were beautiful. We'll never forget the kindness you showed us and we are so appreciative of how smoothly the proceedings went. Thank you, again, Amarantha Hill on behalf of the Hill Family.</request>\n\n        Example 2 Output:\n        <reasoning>User leaves a positive review of their experience.</reasoning>\n        <intent>Support, Feedback, Complaint</intent>\n        </example 2>\n        <example 3>\n\n        ...\n\n        </example 8>\n        <example 9>\n        Example 9 Input:\n        <request>Your website keeps sending ad-popups that block the entire screen. It took me twenty minutes just to finally find the phone number to call and complain. How can I possibly access my account information with all of these popups? Can you access my account for me, since your website is broken? I need to know what the address is on file.</request>\n\n        Example 9 Output:\n        <reasoning>The user requests help accessing their web account information.</reasoning>\n        <intent>Support, Feedback, Complaint</intent>\n        </example 9>\n\n        Remember to always include your classification reasoning before your actual intent output. The reasoning should be enclosed in <reasoning> tags and the intent in <intent> tags. Return only the reasoning and the intent.\n        \"\"\"",
      "language": "python"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Define whether to use Claude for ticket routing",
      "id": "define-whether-to-use-claude-for-ticket-routing"
    },
    {
      "level": "h2",
      "text": "Build and deploy your LLM support workflow",
      "id": "build-and-deploy-your-llm-support-workflow"
    },
    {
      "level": "h3",
      "text": "Understand your current support approach",
      "id": "understand-your-current-support-approach"
    },
    {
      "level": "h3",
      "text": "Define user intent categories",
      "id": "define-user-intent-categories"
    },
    {
      "level": "h3",
      "text": "Establish success criteria",
      "id": "establish-success-criteria"
    },
    {
      "level": "h3",
      "text": "Choose the right Claude model",
      "id": "choose-the-right-claude-model"
    },
    {
      "level": "h3",
      "text": "Build a strong prompt",
      "id": "build-a-strong-prompt"
    },
    {
      "level": "h3",
      "text": "Deploy your prompt",
      "id": "deploy-your-prompt"
    }
  ],
  "url": "llms-txt#ticket-routing",
  "links": []
}