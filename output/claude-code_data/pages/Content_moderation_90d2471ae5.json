{
  "title": "Content moderation",
  "content": "Content moderation is a critical aspect of maintaining a safe, respectful, and productive environment in digital applications. In this guide, we'll discuss how Claude can be used to moderate content within your digital application.\n\n> Visit our [content moderation cookbook](https://platform.claude.com/cookbook/misc-building-moderation-filter) to see an example content moderation implementation using Claude.\n\n<Tip>This guide is focused on moderating user-generated content within your application. If you're looking for guidance on moderating interactions with Claude, please refer to our [guardrails guide](/docs/en/test-and-evaluate/strengthen-guardrails/reduce-hallucinations).</Tip>\n\n## Before building with Claude\n\n### Decide whether to use Claude for content moderation\n\nHere are some key indicators that you should use an LLM like Claude instead of a traditional ML or rules-based approach for content moderation:\n\n<section title=\"You want a cost-effective and rapid implementation\">\nTraditional ML methods require significant engineering resources, ML expertise, and infrastructure costs. Human moderation systems incur even higher costs. With Claude, you can have a sophisticated moderation system up and running in a fraction of the time for a fraction of the price.\n</section>\n<section title=\"You desire both semantic understanding and quick decisions\">\nTraditional ML approaches, such as bag-of-words models or simple pattern matching, often struggle to understand the tone, intent, and context of the content. While human moderation systems excel at understanding semantic meaning, they require time for content to be reviewed. Claude bridges the gap by combining semantic understanding with the ability to deliver moderation decisions quickly.\n</section>\n<section title=\"You need consistent policy decisions\">\nBy leveraging its advanced reasoning capabilities, Claude can interpret and apply complex moderation guidelines uniformly. This consistency helps ensure fair treatment of all content, reducing the risk of inconsistent or biased moderation decisions that can undermine user trust.\n</section>\n<section title=\"Your moderation policies are likely to change or evolve over time\">\nOnce a traditional ML approach has been established, changing it is a laborious and data-intensive undertaking. On the other hand, as your product or customer needs evolve, Claude can easily adapt to changes or additions to moderation policies without extensive relabeling of training data.\n</section>\n<section title=\"You require interpretable reasoning for your moderation decisions\">\nIf you wish to provide users or regulators with clear explanations behind moderation decisions, Claude can generate detailed and coherent justifications. This transparency is important for building trust and ensuring accountability in content moderation practices.\n</section>\n<section title=\"You need multilingual support without maintaining separate models\">\nTraditional ML approaches typically require separate models or extensive translation processes for each supported language. Human moderation requires hiring a workforce fluent in each supported language. Claudeâ€™s multilingual capabilities allow it to classify tickets in various languages without the need for separate models or extensive translation processes, streamlining moderation for global customer bases.\n</section>\n<section title=\"You require multimodal support\">\nClaude's multimodal capabilities allow it to analyze and interpret content across both text and images. This makes it a versatile tool for comprehensive content moderation in environments where different media types need to be evaluated together.\n</section>\n\n<Note>Anthropic has trained all Claude models to be honest, helpful and harmless. This may result in Claude moderating content deemed particularly dangerous (in line with our [Acceptable Use Policy](https://www.anthropic.com/legal/aup)), regardless of the prompt used. For example, an adult website that wants to allow users to post explicit sexual content may find that Claude still flags explicit content as requiring moderation, even if they specify in their prompt not to moderate explicit sexual content. We recommend reviewing our AUP in advance of building a moderation solution.</Note>\n\n### Generate examples of content to moderate\nBefore developing a content moderation solution, first create examples of content that should be flagged and content that should not be flagged. Ensure that you include edge cases and challenging scenarios that may be difficult for a content moderation system to handle effectively. Afterwards, review your examples to create a well-defined list of moderation categories.\nFor instance, the examples generated by a social media platform might include the following:\n\n```python\nallowed_user_comments = [\n    'This movie was great, I really enjoyed it. The main actor really killed it!',\n    'I hate Mondays.',\n    'It is a great time to invest in gold!'\n]\n\ndisallowed_user_comments = [\n    'Delete this post now or you better hide. I am coming after you and your family.',\n    'Stay away from the 5G cellphones!! They are using 5G to control you.',\n    'Congratulations! You have won a $1,000 gift card. Click here to claim your prize!'\n]",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "Before building with Claude",
      "id": "before-building-with-claude"
    },
    {
      "level": "h3",
      "text": "Decide whether to use Claude for content moderation",
      "id": "decide-whether-to-use-claude-for-content-moderation"
    },
    {
      "level": "h3",
      "text": "Generate examples of content to moderate",
      "id": "generate-examples-of-content-to-moderate"
    }
  ],
  "url": "llms-txt#content-moderation",
  "links": []
}