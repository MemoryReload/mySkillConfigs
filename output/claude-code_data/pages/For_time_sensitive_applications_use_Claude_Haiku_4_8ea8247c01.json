{
  "title": "For time-sensitive applications, use Claude Haiku 4.5",
  "content": "message = client.messages.create(\n    model=\"claude-haiku-4-5\",\n    max_tokens=100,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": \"Summarize this customer feedback in 2 sentences: [feedback text]\"\n    }]\n)\n```\n\nFor more details about model metrics, see our [models overview](/docs/en/about-claude/models/overview) page.\n\n### 2. Optimize prompt and output length\n\nMinimize the number of tokens in both your input prompt and the expected output, while still maintaining high performance. The fewer tokens the model has to process and generate, the faster the response will be.\n\nHere are some tips to help you optimize your prompts and outputs:\n\n- **Be clear but concise**: Aim to convey your intent clearly and concisely in the prompt. Avoid unnecessary details or redundant information, while keeping in mind that [claude lacks context](/docs/en/build-with-claude/prompt-engineering/be-clear-and-direct) on your use case and may not make the intended leaps of logic if instructions are unclear.\n- **Ask for shorter responses:**: Ask Claude directly to be concise. The Claude 3 family of models has improved steerability over previous generations. If Claude is outputting unwanted length, ask Claude to [curb its chattiness](/docs/en/build-with-claude/prompt-engineering/be-clear-and-direct).\n  <Tip> Due to how LLMs count [tokens](/docs/en/about-claude/glossary#tokens) instead of words, asking for an exact word count or a word count limit is not as effective a strategy as asking for paragraph or sentence count limits.</Tip>\n- **Set appropriate output limits**: Use the `max_tokens` parameter to set a hard limit on the maximum length of the generated response. This prevents Claude from generating overly long outputs.\n  > **Note**: When the response reaches `max_tokens` tokens, the response will be cut off, perhaps midsentence or mid-word, so this is a blunt technique that may require post-processing and is usually most appropriate for multiple choice or short answer responses where the answer comes right at the beginning.\n- **Experiment with temperature**: The `temperature` [parameter](/docs/en/api/messages) controls the randomness of the output. Lower values (e.g., 0.2) can sometimes lead to more focused and shorter responses, while higher values (e.g., 0.8) may result in more diverse but potentially longer outputs.\n\nFinding the right balance between prompt clarity, output quality, and token count may require some experimentation.\n\n### 3. Leverage streaming\n\nStreaming is a feature that allows the model to start sending back its response before the full output is complete. This can significantly improve the perceived responsiveness of your application, as users can see the model's output in real-time.\n\nWith streaming enabled, you can process the model's output as it arrives, updating your user interface or performing other tasks in parallel. This can greatly enhance the user experience and make your application feel more interactive and responsive.\n\nVisit [streaming Messages](/docs/en/build-with-claude/streaming) to learn about how you can implement streaming for your use case.",
  "code_samples": [],
  "headings": [
    {
      "level": "h3",
      "text": "2. Optimize prompt and output length",
      "id": "2.-optimize-prompt-and-output-length"
    },
    {
      "level": "h3",
      "text": "3. Leverage streaming",
      "id": "3.-leverage-streaming"
    }
  ],
  "url": "llms-txt#for-time-sensitive-applications,-use-claude-haiku-4.5",
  "links": []
}