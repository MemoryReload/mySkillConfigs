{
  "title": "The response will contain summarized thinking blocks and text blocks",
  "content": "for block in response.content:\n    if block.type == \"thinking\":\n        print(f\"\\nThinking summary: {block.thinking}\")\n    elif block.type == \"text\":\n        print(f\"\\nResponse: {block.text}\")\ntypescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic();\n\nconst response = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 16000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 10000\n  },\n  messages: [{\n    role: \"user\",\n    content: \"Are there an infinite number of prime numbers such that n mod 4 == 3?\"\n  }]\n});\n\n// The response will contain summarized thinking blocks and text blocks\nfor (const block of response.content) {\n  if (block.type === \"thinking\") {\n    console.log(`\\nThinking summary: ${block.thinking}`);\n  } else if (block.type === \"text\") {\n    console.log(`\\nResponse: ${block.text}`);\n  }\n}\njava Java\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.models.beta.messages.*;\nimport com.anthropic.models.beta.messages.MessageCreateParams;\nimport com.anthropic.models.messages.*;\n\npublic class SimpleThinkingExample {\n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\nBetaMessage response = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(16000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                        .addUserMessage(\"Are there an infinite number of prime numbers such that n mod 4 == 3?\")\n                        .build()\n        );\n\nSystem.out.println(response);\n    }\n}\nbash Shell\ncurl https://api.anthropic.com/v1/messages \\\n     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n     --header \"anthropic-version: 2023-06-01\" \\\n     --header \"content-type: application/json\" \\\n     --data \\\n'{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 16000,\n    \"stream\": true,\n    \"thinking\": {\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is 27 * 453?\"\n        }\n    ]\n}'\npython Python\nimport anthropic\n\nclient = anthropic.Anthropic()\n\nwith client.messages.stream(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=16000,\n    thinking={\"type\": \"enabled\", \"budget_tokens\": 10000},\n    messages=[{\"role\": \"user\", \"content\": \"What is 27 * 453?\"}],\n) as stream:\n    thinking_started = False\n    response_started = False\n\nfor event in stream:\n        if event.type == \"content_block_start\":\n            print(f\"\\nStarting {event.content_block.type} block...\")\n            # Reset flags for each new block\n            thinking_started = False\n            response_started = False\n        elif event.type == \"content_block_delta\":\n            if event.delta.type == \"thinking_delta\":\n                if not thinking_started:\n                    print(\"Thinking: \", end=\"\", flush=True)\n                    thinking_started = True\n                print(event.delta.thinking, end=\"\", flush=True)\n            elif event.delta.type == \"text_delta\":\n                if not response_started:\n                    print(\"Response: \", end=\"\", flush=True)\n                    response_started = True\n                print(event.delta.text, end=\"\", flush=True)\n        elif event.type == \"content_block_stop\":\n            print(\"\\nBlock complete.\")\ntypescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic();\n\nconst stream = await client.messages.stream({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 16000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 10000\n  },\n  messages: [{\n    role: \"user\",\n    content: \"What is 27 * 453?\"\n  }]\n});\n\nlet thinkingStarted = false;\nlet responseStarted = false;\n\nfor await (const event of stream) {\n  if (event.type === 'content_block_start') {\n    console.log(`\\nStarting ${event.content_block.type} block...`);\n    // Reset flags for each new block\n    thinkingStarted = false;\n    responseStarted = false;\n  } else if (event.type === 'content_block_delta') {\n    if (event.delta.type === 'thinking_delta') {\n      if (!thinkingStarted) {\n        process.stdout.write('Thinking: ');\n        thinkingStarted = true;\n      }\n      process.stdout.write(event.delta.thinking);\n    } else if (event.delta.type === 'text_delta') {\n      if (!responseStarted) {\n        process.stdout.write('Response: ');\n        responseStarted = true;\n      }\n      process.stdout.write(event.delta.text);\n    }\n  } else if (event.type === 'content_block_stop') {\n    console.log('\\nBlock complete.');\n  }\n}\njava Java\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.core.http.StreamResponse;\nimport com.anthropic.models.beta.messages.MessageCreateParams;\nimport com.anthropic.models.beta.messages.BetaRawMessageStreamEvent;\nimport com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;\nimport com.anthropic.models.messages.Model;\n\npublic class SimpleThinkingStreamingExample {\n    private static boolean thinkingStarted = false;\n    private static boolean responseStarted = false;\n    \n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\nMessageCreateParams createParams = MessageCreateParams.builder()\n                .model(Model.CLAUDE_OPUS_4_0)\n                .maxTokens(16000)\n                .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                .addUserMessage(\"What is 27 * 453?\")\n                .build();\n\ntry (StreamResponse<BetaRawMessageStreamEvent> streamResponse =\n                     client.beta().messages().createStreaming(createParams)) {\n            streamResponse.stream()\n                    .forEach(event -> {\n                        if (event.isContentBlockStart()) {\n                            System.out.printf(\"\\nStarting %s block...%n\",\n                                    event.asContentBlockStart()._type());\n                            // Reset flags for each new block\n                            thinkingStarted = false;\n                            responseStarted = false;\n                        } else if (event.isContentBlockDelta()) {\n                            var delta = event.asContentBlockDelta().delta();\n                            if (delta.isBetaThinking()) {\n                                if (!thinkingStarted) {\n                                    System.out.print(\"Thinking: \");\n                                    thinkingStarted = true;\n                                }\n                                System.out.print(delta.asBetaThinking().thinking());\n                                System.out.flush();\n                            } else if (delta.isBetaText()) {\n                                if (!responseStarted) {\n                                    System.out.print(\"Response: \");\n                                    responseStarted = true;\n                                }\n                                System.out.print(delta.asBetaText().text());\n                                System.out.flush();\n                            }\n                        } else if (event.isContentBlockStop()) {\n                            System.out.println(\"\\nBlock complete.\");\n                        }\n                    });\n        }\n    }\n}\njson\nevent: message_start\ndata: {\"type\": \"message_start\", \"message\": {\"id\": \"msg_01...\", \"type\": \"message\", \"role\": \"assistant\", \"content\": [], \"model\": \"claude-sonnet-4-5\", \"stop_reason\": null, \"stop_sequence\": null}}\n\nevent: content_block_start\ndata: {\"type\": \"content_block_start\", \"index\": 0, \"content_block\": {\"type\": \"thinking\", \"thinking\": \"\"}}\n\nevent: content_block_delta\ndata: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"thinking_delta\", \"thinking\": \"Let me solve this step by step:\\n\\n1. First break down 27 * 453\"}}\n\nevent: content_block_delta\ndata: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"thinking_delta\", \"thinking\": \"\\n2. 453 = 400 + 50 + 3\"}}\n\n// Additional thinking deltas...\n\nevent: content_block_delta\ndata: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"signature_delta\", \"signature\": \"EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds...\"}}\n\nevent: content_block_stop\ndata: {\"type\": \"content_block_stop\", \"index\": 0}\n\nevent: content_block_start\ndata: {\"type\": \"content_block_start\", \"index\": 1, \"content_block\": {\"type\": \"text\", \"text\": \"\"}}\n\nevent: content_block_delta\ndata: {\"type\": \"content_block_delta\", \"index\": 1, \"delta\": {\"type\": \"text_delta\", \"text\": \"27 * 453 = 12,231\"}}\n\n// Additional text deltas...\n\nevent: content_block_stop\ndata: {\"type\": \"content_block_stop\", \"index\": 1}\n\nevent: message_delta\ndata: {\"type\": \"message_delta\", \"delta\": {\"stop_reason\": \"end_turn\", \"stop_sequence\": null}}\n\nevent: message_stop\ndata: {\"type\": \"message_stop\"}\n\nUser: \"What's the weather in Paris?\"\nAssistant: [thinking] + [tool_use: get_weather]\nUser: [tool_result: \"20°C, sunny\"]\nAssistant: [text: \"The weather in Paris is 20°C and sunny\"]\n\nExpected `thinking` or `redacted_thinking`, but found `tool_use`.\nWhen `thinking` is enabled, a final `assistant` message must start\nwith a thinking block (preceding the lastmost set of `tool_use` and\n`tool_result` blocks).\n\nUser: \"What's the weather?\"\nAssistant: [tool_use] (thinking disabled)\nUser: [tool_result]\n// Cannot enable thinking here - still in the same assistant turn\n\nUser: \"What's the weather?\"\nAssistant: [tool_use] (thinking disabled)\nUser: [tool_result]\nAssistant: [text: \"It's sunny\"] \nUser: \"What about tomorrow?\" (thinking disabled)\nAssistant: [thinking] + [text: \"...\"] (thinking enabled - new turn)\npython Python\nweather_tool = {\n    \"name\": \"get_weather\",\n    \"description\": \"Get current weather for a location\",\n    \"input_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\"type\": \"string\"}\n        },\n        \"required\": [\"location\"]\n    }\n}",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nTo turn on extended thinking, add a `thinking` object, with the `type` parameter set to `enabled` and the `budget_tokens` to a specified token budget for extended thinking.\n\nThe `budget_tokens` parameter determines the maximum number of tokens Claude is allowed to use for its internal reasoning process. In Claude 4 models, this limit applies to full thinking tokens, and not to [the summarized output](#summarized-thinking). Larger budgets can improve response quality by enabling more thorough analysis for complex problems, although Claude may not use the entire budget allocated, especially at ranges above 32k.\n\n`budget_tokens` must be set to a value less than `max_tokens`. However, when using [interleaved thinking with tools](#interleaved-thinking), you can exceed this limit as the token limit becomes your entire context window (200k tokens). \n\n### Summarized thinking\n\nWith extended thinking enabled, the Messages API for Claude 4 models returns a summary of Claude's full thinking process. Summarized thinking provides the full intelligence benefits of extended thinking, while preventing misuse.\n\nHere are some important considerations for summarized thinking:\n\n- You're charged for the full thinking tokens generated by the original request, not the summary tokens.\n- The billed output token count will **not match** the count of tokens you see in the response.\n- The first few lines of thinking output are more verbose, providing detailed reasoning that's particularly helpful for prompt engineering purposes. \n- As Anthropic seeks to improve the extended thinking feature, summarization behavior is subject to change.\n- Summarization preserves the key ideas of Claude's thinking process with minimal added latency, enabling a streamable user experience and easy migration from Claude Sonnet 3.7 to Claude 4 models.\n- Summarization is processed by a different model than the one you target in your requests. The thinking model does not see the summarized output.\n\n<Note>\nClaude Sonnet 3.7 continues to return full thinking output.\n\nIn rare cases where you need access to full thinking output for Claude 4 models, [contact our sales team](mailto:sales@anthropic.com).\n</Note>\n\n### Streaming thinking\n\nYou can stream extended thinking responses using [server-sent events (SSE)](https://developer.mozilla.org/en-US/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents).\n\nWhen streaming is enabled for extended thinking, you receive thinking content via `thinking_delta` events.\n\nFor more documention on streaming via the Messages API, see [Streaming Messages](/docs/en/build-with-claude/streaming).\n\nHere's how to handle streaming with thinking:\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n<TryInConsoleButton userPrompt=\"What is 27 * 453?\" thinkingBudgetTokens={16000}>\n  Try in Console\n</TryInConsoleButton>\n  \n\nExample streaming output:",
      "language": "unknown"
    },
    {
      "code": "<Note>\nWhen using streaming with thinking enabled, you might notice that text sometimes arrives in larger chunks alternating with smaller, token-by-token delivery. This is expected behavior, especially for thinking content.\n\nThe streaming system needs to process content in batches for optimal performance, which can result in this \"chunky\" delivery pattern, with possible delays between streaming events. We're continuously working to improve this experience, with future updates focused on making thinking content stream more smoothly.\n</Note>\n\n## Extended thinking with tool use\n\nExtended thinking can be used alongside [tool use](/docs/en/agents-and-tools/tool-use/overview), allowing Claude to reason through tool selection and results processing.\n\nWhen using extended thinking with tool use, be aware of the following limitations:\n\n1. **Tool choice limitation**: Tool use with thinking only supports `tool_choice: {\"type\": \"auto\"}` (the default) or `tool_choice: {\"type\": \"none\"}`. Using `tool_choice: {\"type\": \"any\"}` or `tool_choice: {\"type\": \"tool\", \"name\": \"...\"}` will result in an error because these options force tool use, which is incompatible with extended thinking.\n\n2. **Preserving thinking blocks**: During tool use, you must pass `thinking` blocks back to the API for the last assistant message. Include the complete unmodified block back to the API to maintain reasoning continuity.\n\n### Toggling thinking modes in conversations\n\nYou cannot toggle thinking in the middle of an assistant turn, including during tool use loops. The entire assistant turn must operate in a single thinking mode: \n\n- **If thinking is enabled**, the final assistant turn must start with a thinking block.\n- **If thinking is disabled**, the final assistant turn must not contain any thinking blocks\n\nFrom the model's perspective, **tool use loops are part of the assistant turn**. An assistant turn doesn't complete until Claude finishes its full response, which may include multiple tool calls and results.\n\nFor example, this sequence is all part of a **single assistant turn**:",
      "language": "unknown"
    },
    {
      "code": "Even though there are multiple API messages, the tool use loop is conceptually part of one continuous assistant response.\n\n#### Common error scenarios\n\nYou might encounter this error:",
      "language": "unknown"
    },
    {
      "code": "This typically occurs when:\n1. You had thinking **disabled** during a tool use sequence\n2. You want to enable thinking again\n3. Your last assistant message contains tool use blocks but no thinking block\n\n#### Practical guidance\n\n**✗ Invalid: Toggling thinking immediately after tool use**",
      "language": "unknown"
    },
    {
      "code": "**✓ Valid: Complete the assistant turn first**",
      "language": "unknown"
    },
    {
      "code": "**Best practice**: Plan your thinking strategy at the start of each turn rather than trying to toggle mid-turn.\n\n<Note>\nToggling thinking modes also invalidates prompt caching for message history. For more details, see the [Extended thinking with prompt caching](#extended-thinking-with-prompt-caching) section.\n</Note>\n\n<section title=\"Example: Passing thinking blocks with tool results\">\n\nHere's a practical example showing how to preserve thinking blocks when providing tool results:\n\n<CodeGroup>",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Summarized thinking",
      "id": "summarized-thinking"
    },
    {
      "level": "h3",
      "text": "Streaming thinking",
      "id": "streaming-thinking"
    },
    {
      "level": "h2",
      "text": "Extended thinking with tool use",
      "id": "extended-thinking-with-tool-use"
    },
    {
      "level": "h3",
      "text": "Toggling thinking modes in conversations",
      "id": "toggling-thinking-modes-in-conversations"
    }
  ],
  "url": "llms-txt#the-response-will-contain-summarized-thinking-blocks-and-text-blocks",
  "links": []
}