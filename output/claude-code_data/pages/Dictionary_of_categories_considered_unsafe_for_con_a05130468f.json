{
  "title": "Dictionary of categories considered unsafe for content moderation, with their definitions",
  "content": "unsafe_category_definitions = {\n    'Child Exploitation': 'Content that depicts child nudity or that enables, encourages, excuses, or depicts the sexual abuse of children.',\n    'Conspiracy Theories': \"\"\"Content that promotes or endorses unfounded, false, or misleading theories about events, situations, or individuals, \n        which can incite harm or undermine public trust in institutions or facts.\"\"\",\n    'Hate': \"\"\"Content that is hateful toward people on the basis of their protected characteristics \n        (race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity and serious disease), \n        or content that perpetuates negative stereotypes.\"\"\",\n    'Indiscriminate Weapons': 'Content that enables the creation of weapons of mass destruction.', \n    'Intellectual Property': 'Content that violates the intellectual property rights of third parties.',\n    'Non-Violent Crimes': 'Content that enables, encourages, or excuses the commission of non-violent crimes.', \n    'Privacy': 'Content that contains sensitive, personal information about private individuals.',\n    'Self-Harm': 'Content that enables, encourages, or excuses acts of intentional self-harm.',\n    'Sex Crimes': 'Content that enables, encourages, or excuses the commission of sex crimes.',\n    'Sexual Content': 'Content that contains erotic descriptions or explicit depictions of sex acts.',\n    'Specialized Advice': \"\"\"Content that contains financial, medical, or legal advice. \n        Financial advice includes guidance on investments, stocks, bonds, or any financial planning.\"\"\",\n    'Violent Crimes': 'Content that enables, encourages, or excuses the commission of violent crimes.', \n}\n\ndef moderate_message_with_definitions(message, unsafe_category_definitions):\n    # Format unsafe categories string, with each category and its definition on a new line\n    unsafe_category_str = '\\n'.join(f'{category}: {definition}' \n        for category, definition in unsafe_category_definitions.items()\n    )\n    \n    # Construct the prompt for Claude, including the message and unsafe categories\n    assessment_prompt = f\"\"\"Determine whether the following message warrants moderation, based on the unsafe categories outlined below.\n\nMessage:\n<message>{message}</message>\n\nUnsafe Categories and Their Definitions:\n<categories>\n{unsafe_category_str}\n</categories>\n\nIt's important that you remember all unsafe categories and their definitions.\n\nRespond with ONLY a JSON object, using the format below:\n{{\n  \"violation\": <Boolean field denoting whether the message should be moderated>,\n  \"categories\": [Comma-separated list of violated categories],\n  \"explanation\": [Optional. Only include if there is a violation.]\n}}\"\"\"\n\n# Send the request to Claude for content moderation\n    response = client.messages.create(\n        model=\"claude-3-haiku-20240307\",  # Using the Haiku model for lower costs\n        max_tokens=200,\n        temperature=0,   # Use 0 temperature for increased consistency\n        messages=[\n            {\"role\": \"user\", \"content\": assessment_prompt}\n        ]\n    )\n    \n    # Parse the JSON response from Claude\n    assessment = json.loads(response.content[0].text)\n    \n    # Extract the violation status from the assessment\n    contains_violation = assessment['violation']\n    \n    # If there's a violation, get the categories and explanation; otherwise, use empty defaults\n    violated_categories = assessment.get('categories', []) if contains_violation else []\n    explanation = assessment.get('explanation') if contains_violation else None\n    \n    return contains_violation, violated_categories, explanation",
  "code_samples": [],
  "headings": [],
  "url": "llms-txt#dictionary-of-categories-considered-unsafe-for-content-moderation,-with-their-definitions",
  "links": []
}