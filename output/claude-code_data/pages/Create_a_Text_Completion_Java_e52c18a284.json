{
  "title": "Create a Text Completion (Java)",
  "content": "URL: https://platform.claude.com/docs/en/api/java/completions/create\n\n`Completion completions().create(CompletionCreateParamsparams, RequestOptionsrequestOptions = RequestOptions.none())`\n\n**post** `/v1/complete`\n\n[Legacy] Create a Text Completion.\n\nThe Text Completions API is a legacy API. We recommend using the [Messages API](https://docs.claude.com/en/api/messages) going forward.\n\nFuture models and features will not be compatible with Text Completions. See our [migration guide](https://docs.claude.com/en/api/migrating-from-text-completions-to-messages) for guidance in migrating from Text Completions to Messages.\n\n- `CompletionCreateParams params`\n\n- `Optional<List<AnthropicBeta>> betas`\n\nOptional header to specify the beta version(s) you want to use.\n\n- `MESSAGE_BATCHES_2024_09_24(\"message-batches-2024-09-24\")`\n\n- `PROMPT_CACHING_2024_07_31(\"prompt-caching-2024-07-31\")`\n\n- `COMPUTER_USE_2024_10_22(\"computer-use-2024-10-22\")`\n\n- `COMPUTER_USE_2025_01_24(\"computer-use-2025-01-24\")`\n\n- `PDFS_2024_09_25(\"pdfs-2024-09-25\")`\n\n- `TOKEN_COUNTING_2024_11_01(\"token-counting-2024-11-01\")`\n\n- `TOKEN_EFFICIENT_TOOLS_2025_02_19(\"token-efficient-tools-2025-02-19\")`\n\n- `OUTPUT_128K_2025_02_19(\"output-128k-2025-02-19\")`\n\n- `FILES_API_2025_04_14(\"files-api-2025-04-14\")`\n\n- `MCP_CLIENT_2025_04_04(\"mcp-client-2025-04-04\")`\n\n- `MCP_CLIENT_2025_11_20(\"mcp-client-2025-11-20\")`\n\n- `DEV_FULL_THINKING_2025_05_14(\"dev-full-thinking-2025-05-14\")`\n\n- `INTERLEAVED_THINKING_2025_05_14(\"interleaved-thinking-2025-05-14\")`\n\n- `CODE_EXECUTION_2025_05_22(\"code-execution-2025-05-22\")`\n\n- `EXTENDED_CACHE_TTL_2025_04_11(\"extended-cache-ttl-2025-04-11\")`\n\n- `CONTEXT_1M_2025_08_07(\"context-1m-2025-08-07\")`\n\n- `CONTEXT_MANAGEMENT_2025_06_27(\"context-management-2025-06-27\")`\n\n- `MODEL_CONTEXT_WINDOW_EXCEEDED_2025_08_26(\"model-context-window-exceeded-2025-08-26\")`\n\n- `SKILLS_2025_10_02(\"skills-2025-10-02\")`\n\n- `long maxTokensToSample`\n\nThe maximum number of tokens to generate before stopping.\n\nNote that our models may stop _before_ reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.\n\nThe model that will complete your prompt.\n\nSee [models](https://docs.anthropic.com/en/docs/models-overview) for additional details and options.\n\nThe prompt that you want Claude to complete.\n\nFor proper response generation you will need to format your prompt using alternating `\n\nAssistant:` conversational turns. For example:\n\nSee [prompt validation](https://docs.claude.com/en/api/prompt-validation) and our guide to [prompt design](https://docs.claude.com/en/docs/intro-to-prompting) for more details.\n\n- `Optional<Metadata> metadata`\n\nAn object describing metadata about the request.\n\n- `Optional<List<String>> stopSequences`\n\nSequences that will cause the model to stop generating.\n\nOur models stop on `\"\n\nHuman:\"`, and may include additional built-in stop sequences in the future. By providing the stop_sequences parameter, you may include additional strings that will cause the model to stop generating.\n\n- `Optional<Double> temperature`\n\nAmount of randomness injected into the response.\n\nDefaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to `1.0` for creative and generative tasks.\n\nNote that even with `temperature` of `0.0`, the results will not be fully deterministic.\n\n- `Optional<Long> topK`\n\nOnly sample from the top K options for each subsequent token.\n\nUsed to remove \"long tail\" low probability responses. [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).\n\nRecommended for advanced use cases only. You usually only need to use `temperature`.\n\n- `Optional<Double> topP`\n\nUse nucleus sampling.\n\nIn nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by `top_p`. You should either alter `temperature` or `top_p`, but not both.\n\nRecommended for advanced use cases only. You usually only need to use `temperature`.\n\n- `class Completion:`\n\nUnique object identifier.\n\nThe format and length of IDs may change over time.\n\n- `String completion`\n\nThe resulting completion up to and excluding the stop sequences.\n\nThe model that will complete your prompt.\n\nSee [models](https://docs.anthropic.com/en/docs/models-overview) for additional details and options.\n\n- `CLAUDE_OPUS_4_5_20251101(\"claude-opus-4-5-20251101\")`\n\nPremium model combining maximum intelligence with practical performance\n\n- `CLAUDE_OPUS_4_5(\"claude-opus-4-5\")`\n\nPremium model combining maximum intelligence with practical performance\n\n- `CLAUDE_3_7_SONNET_LATEST(\"claude-3-7-sonnet-latest\")`\n\nHigh-performance model with early extended thinking\n\n- `CLAUDE_3_7_SONNET_20250219(\"claude-3-7-sonnet-20250219\")`\n\nHigh-performance model with early extended thinking\n\n- `CLAUDE_3_5_HAIKU_LATEST(\"claude-3-5-haiku-latest\")`\n\nFastest and most compact model for near-instant responsiveness\n\n- `CLAUDE_3_5_HAIKU_20241022(\"claude-3-5-haiku-20241022\")`\n\n- `CLAUDE_HAIKU_4_5(\"claude-haiku-4-5\")`\n\nHybrid model, capable of near-instant responses and extended thinking\n\n- `CLAUDE_HAIKU_4_5_20251001(\"claude-haiku-4-5-20251001\")`\n\nHybrid model, capable of near-instant responses and extended thinking\n\n- `CLAUDE_SONNET_4_20250514(\"claude-sonnet-4-20250514\")`\n\nHigh-performance model with extended thinking\n\n- `CLAUDE_SONNET_4_0(\"claude-sonnet-4-0\")`\n\nHigh-performance model with extended thinking\n\n- `CLAUDE_4_SONNET_20250514(\"claude-4-sonnet-20250514\")`\n\nHigh-performance model with extended thinking\n\n- `CLAUDE_SONNET_4_5(\"claude-sonnet-4-5\")`\n\nOur best model for real-world agents and coding\n\n- `CLAUDE_SONNET_4_5_20250929(\"claude-sonnet-4-5-20250929\")`\n\nOur best model for real-world agents and coding\n\n- `CLAUDE_OPUS_4_0(\"claude-opus-4-0\")`\n\nOur most capable model\n\n- `CLAUDE_OPUS_4_20250514(\"claude-opus-4-20250514\")`\n\nOur most capable model\n\n- `CLAUDE_4_OPUS_20250514(\"claude-4-opus-20250514\")`\n\nOur most capable model\n\n- `CLAUDE_OPUS_4_1_20250805(\"claude-opus-4-1-20250805\")`\n\nOur most capable model\n\n- `CLAUDE_3_OPUS_LATEST(\"claude-3-opus-latest\")`\n\nExcels at writing and complex tasks\n\n- `CLAUDE_3_OPUS_20240229(\"claude-3-opus-20240229\")`\n\nExcels at writing and complex tasks\n\n- `CLAUDE_3_HAIKU_20240307(\"claude-3-haiku-20240307\")`\n\nOur previous most fast and cost-effective\n\n- `Optional<String> stopReason`\n\nThe reason that we stopped.\n\nThis may be one the following values:\n\n* `\"stop_sequence\"`: we reached a stop sequence â€” either provided by you via the `stop_sequences` parameter, or a stop sequence built into the model\n    * `\"max_tokens\"`: we exceeded `max_tokens_to_sample` or the model's maximum\n\n- `JsonValue; type \"completion\"constant`\n\nFor Text Completions, this is always `\"completion\"`.\n\n- `COMPLETION(\"completion\")`",
  "code_samples": [
    {
      "code": "\"\n    \n    Human: {userQuestion}\n    \n    Assistant:\"",
      "language": "unknown"
    },
    {
      "code": "package com.anthropic.example;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.models.completions.Completion;\nimport com.anthropic.models.completions.CompletionCreateParams;\nimport com.anthropic.models.messages.Model;\n\npublic final class Main {\n    private Main() {}\n\n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n        CompletionCreateParams params = CompletionCreateParams.builder()\n            .maxTokensToSample(256L)\n            .model(Model.CLAUDE_OPUS_4_5_20251101)\n            .prompt(\"\\n\\nHuman: Hello, world!\\n\\nAssistant:\")\n            .build();\n        Completion completion = client.completions().create(params);\n    }\n}",
      "language": "java"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Create",
      "id": "create"
    },
    {
      "level": "h3",
      "text": "Parameters",
      "id": "parameters"
    },
    {
      "level": "h3",
      "text": "Returns",
      "id": "returns"
    },
    {
      "level": "h3",
      "text": "Example",
      "id": "example"
    }
  ],
  "url": "llms-txt#create-a-text-completion-(java)",
  "links": []
}