{
  "title": "Reducing latency",
  "content": "Latency refers to the time it takes for the model to process a prompt and and generate an output. Latency can be influenced by various factors, such as the size of the model, the complexity of the prompt, and the underlying infrastructure supporting the model and point of interaction.\n\n<Note>\nIt's always better to first engineer a prompt that works well without model or prompt constraints, and then try latency reduction strategies afterward. Trying to reduce latency prematurely might prevent you from discovering what top performance looks like.\n</Note>\n\n## How to measure latency\n\nWhen discussing latency, you may come across several terms and measurements:\n\n- **Baseline latency**: This is the time taken by the model to process the prompt and generate the response, without considering the input and output tokens per second. It provides a general idea of the model's speed.\n- **Time to first token (TTFT)**: This metric measures the time it takes for the model to generate the first token of the response, from when the prompt was sent. It's particularly relevant when you're using streaming (more on that later) and want to provide a responsive experience to your users.\n\nFor a more in-depth understanding of these terms, check out our [glossary](/docs/en/about-claude/glossary).\n\n## How to reduce latency\n\n### 1. Choose the right model\n\nOne of the most straightforward ways to reduce latency is to select the appropriate model for your use case. Anthropic offers a [range of models](/docs/en/about-claude/models/overview) with different capabilities and performance characteristics. Consider your specific requirements and choose the model that best fits your needs in terms of speed and output quality.\n\nFor speed-critical applications, **Claude Haiku 4.5** offers the fastest response times while maintaining high intelligence:\n\n```python\nimport anthropic\n\nclient = anthropic.Anthropic()",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "How to measure latency",
      "id": "how-to-measure-latency"
    },
    {
      "level": "h2",
      "text": "How to reduce latency",
      "id": "how-to-reduce-latency"
    },
    {
      "level": "h3",
      "text": "1. Choose the right model",
      "id": "1.-choose-the-right-model"
    }
  ],
  "url": "llms-txt#reducing-latency",
  "links": []
}