{
  "title": "Third request - different thinking parameters (cache miss for messages)",
  "content": "print(\"\\nThird request - different thinking parameters (cache miss for messages)\")\nresponse3 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 8000  # Changed thinking budget\n    },\n    system=SYSTEM_PROMPT,  # System prompt remains cached\n    messages=MESSAGES  # Messages cache is invalidated\n)\n\nprint(f\"Third response usage: {response3.usage}\")\ntypescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\nimport axios from 'axios';\nimport * as cheerio from 'cheerio';\n\nconst client = new Anthropic();\n\nasync function fetchArticleContent(url: string): Promise<string> {\n  const response = await axios.get(url);\n  const $ = cheerio.load(response.data);\n  \n  // Remove script and style elements\n  $('script, style').remove();\n  \n  // Get text\n  let text = $.text();\n  \n  // Break into lines and remove leading and trailing space on each\n  const lines = text.split('\\n').map(line => line.trim());\n  // Drop blank lines\n  text = lines.filter(line => line.length > 0).join('\\n');\n  \n  return text;\n}\n\n// Fetch the content of the article\nconst bookUrl = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\";\nconst bookContent = await fetchArticleContent(bookUrl);\n// Use just enough text for caching (first few chapters)\nconst LARGE_TEXT = bookContent.slice(0, 5000);\n\nconst SYSTEM_PROMPT = [\n  {\n    type: \"text\",\n    text: \"You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.\",\n  },\n  {\n    type: \"text\",\n    text: LARGE_TEXT,\n    cache_control: { type: \"ephemeral\" }\n  }\n];\n\nconst MESSAGES = [\n  {\n    role: \"user\",\n    content: \"Analyze the tone of this passage.\"\n  }\n];\n\n// First request - establish cache\nconsole.log(\"First request - establishing cache\");\nconst response1 = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 20000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 4000\n  },\n  system: SYSTEM_PROMPT,\n  messages: MESSAGES\n});\n\nconsole.log(`First response usage: ${response1.usage}`);\n\nMESSAGES.push({\n  role: \"assistant\",\n  content: response1.content\n});\nMESSAGES.push({\n  role: \"user\",\n  content: \"Analyze the characters in this passage.\"\n});\n\n// Second request - same thinking parameters (cache hit expected)\nconsole.log(\"\\nSecond request - same thinking parameters (cache hit expected)\");\nconst response2 = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 20000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 4000\n  },\n  system: SYSTEM_PROMPT,\n  messages: MESSAGES\n});\n\nconsole.log(`Second response usage: ${response2.usage}`);\n\n// Third request - different thinking parameters (cache miss for messages)\nconsole.log(\"\\nThird request - different thinking parameters (cache miss for messages)\");\nconst response3 = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 20000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 8000  // Changed thinking budget\n  },\n  system: SYSTEM_PROMPT,  // System prompt remains cached\n  messages: MESSAGES  // Messages cache is invalidated\n});\n\nconsole.log(`Third response usage: ${response3.usage}`);\npython Python\nfrom anthropic import Anthropic\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef fetch_article_content(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n# Remove script and style elements\n    for script in soup([\"script\", \"style\"]):\n        script.decompose()\n\n# Get text\n    text = soup.get_text()\n\n# Break into lines and remove leading and trailing space on each\n    lines = (line.strip() for line in text.splitlines())\n    # Break multi-headlines into a line each\n    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n    # Drop blank lines\n    text = '\\n'.join(chunk for chunk in chunks if chunk)",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\n</section>\n<section title=\"Messages caching (invalidated when thinking changes)\">\n\n<CodeGroup>",
      "language": "unknown"
    }
  ],
  "headings": [],
  "url": "llms-txt#third-request---different-thinking-parameters-(cache-miss-for-messages)",
  "links": []
}