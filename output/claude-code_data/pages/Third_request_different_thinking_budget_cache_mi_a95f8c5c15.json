{
  "title": "Third request - different thinking budget (cache miss expected)",
  "content": "print(\"\\nThird request - different thinking budget (cache miss expected)\")\nresponse3 = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=20000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 8000  # Different thinking budget breaks cache\n    },\n    messages=MESSAGES\n)\n\nprint(f\"Third response usage: {response3.usage}\")\ntypescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\nimport axios from 'axios';\nimport * as cheerio from 'cheerio';\n\nconst client = new Anthropic();\n\nasync function fetchArticleContent(url: string): Promise<string> {\n  const response = await axios.get(url);\n  const $ = cheerio.load(response.data);\n\n// Remove script and style elements\n  $('script, style').remove();\n\n// Get text\n  let text = $.text();\n\n// Clean up text (break into lines, remove whitespace)\n  const lines = text.split('\\n').map(line => line.trim());\n  const chunks = lines.flatMap(line => line.split('  ').map(phrase => phrase.trim()));\n  text = chunks.filter(chunk => chunk).join('\\n');\n\nasync function main() {\n  // Fetch the content of the article\n  const bookUrl = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\";\n  const bookContent = await fetchArticleContent(bookUrl);\n  // Use just enough text for caching (first few chapters)\n  const LARGE_TEXT = bookContent.substring(0, 5000);\n\n// No system prompt - caching in messages instead\n  let MESSAGES = [\n    {\n      role: \"user\",\n      content: [\n        {\n          type: \"text\",\n          text: LARGE_TEXT,\n          cache_control: {type: \"ephemeral\"},\n        },\n        {\n          type: \"text\",\n          text: \"Analyze the tone of this passage.\"\n        }\n      ]\n    }\n  ];\n\n// First request - establish cache\n  console.log(\"First request - establishing cache\");\n  const response1 = await client.messages.create({\n    model: \"claude-sonnet-4-5\",\n    max_tokens: 20000,\n    thinking: {\n      type: \"enabled\",\n      budget_tokens: 4000\n    },\n    messages: MESSAGES\n  });\n\nconsole.log(`First response usage: `, response1.usage);\n\nMESSAGES = [\n    ...MESSAGES,\n    {\n      role: \"assistant\",\n      content: response1.content\n    },\n    {\n      role: \"user\",\n      content: \"Analyze the characters in this passage.\"\n    }\n  ];\n\n// Second request - same thinking parameters (cache hit expected)\n  console.log(\"\\nSecond request - same thinking parameters (cache hit expected)\");\n  const response2 = await client.messages.create({\n    model: \"claude-sonnet-4-5\",\n    max_tokens: 20000,\n    thinking: {\n      type: \"enabled\",\n      budget_tokens: 4000  // Same thinking budget\n    },\n    messages: MESSAGES\n  });\n\nconsole.log(`Second response usage: `, response2.usage);\n\nMESSAGES = [\n    ...MESSAGES,\n    {\n      role: \"assistant\",\n      content: response2.content\n    },\n    {\n      role: \"user\",\n      content: \"Analyze the setting in this passage.\"\n    }\n  ];\n\n// Third request - different thinking budget (cache miss expected)\n  console.log(\"\\nThird request - different thinking budget (cache miss expected)\");\n  const response3 = await client.messages.create({\n    model: \"claude-sonnet-4-5\",\n    max_tokens: 20000,\n    thinking: {\n      type: \"enabled\",\n      budget_tokens: 8000  // Different thinking budget breaks cache\n    },\n    messages: MESSAGES\n  });\n\nconsole.log(`Third response usage: `, response3.usage);\n}\n\nmain().catch(console.error);\njava Java\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.net.URL;\nimport java.util.Arrays;\nimport java.util.regex.Pattern;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.models.beta.messages.*;\nimport com.anthropic.models.beta.messages.MessageCreateParams;\nimport com.anthropic.models.messages.Model;\n\nimport static java.util.stream.Collectors.joining;\nimport static java.util.stream.Collectors.toList;\n\npublic class ThinkingCacheExample {\n    public static void main(String[] args) throws IOException {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n// Fetch the content of the article\n        String bookUrl = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\";\n        String bookContent = fetchArticleContent(bookUrl);\n        // Use just enough text for caching (first few chapters)\n        String largeText = bookContent.substring(0, 5000);\n\nList<BetaTextBlockParam> systemPrompt = List.of(\n                BetaTextBlockParam.builder()\n                        .text(\"You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.\")\n                        .build(),\n                BetaTextBlockParam.builder()\n                        .text(largeText)\n                        .cacheControl(BetaCacheControlEphemeral.builder().build())\n                        .build()\n        );\n\nList<BetaMessageParam> messages = new ArrayList<>();\n        messages.add(BetaMessageParam.builder()\n                .role(BetaMessageParam.Role.USER)\n                .content(\"Analyze the tone of this passage.\")\n                .build());\n\n// First request - establish cache\n        System.out.println(\"First request - establishing cache\");\n        BetaMessage response1 = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(20000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())\n                        .systemOfBetaTextBlockParams(systemPrompt)\n                        .messages(messages)\n                        .build()\n        );\n\nSystem.out.println(\"First response usage: \" + response1.usage());\n\n// Second request - same thinking parameters (cache hit expected)\n        System.out.println(\"\\nSecond request - same thinking parameters (cache hit expected)\");\n        BetaMessage response2 = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(20000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())\n                        .systemOfBetaTextBlockParams(systemPrompt)\n                        .addMessage(response1)\n                        .addUserMessage(\"Analyze the characters in this passage.\")\n                        .messages(messages)\n                        .build()\n        );\n\nSystem.out.println(\"Second response usage: \" + response2.usage());\n\n// Third request - different thinking budget (cache hit expected because system prompt caching)\n        System.out.println(\"\\nThird request - different thinking budget (cache hit expected)\");\n        BetaMessage response3 = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(20000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(8000).build())\n                        .systemOfBetaTextBlockParams(systemPrompt)\n                        .addMessage(response1)\n                        .addUserMessage(\"Analyze the characters in this passage.\")\n                        .addMessage(response2)\n                        .addUserMessage(\"Analyze the setting in this passage.\")\n                        .build()\n        );\n\nSystem.out.println(\"Third response usage: \" + response3.usage());\n    }\n\nprivate static String fetchArticleContent(String url) throws IOException {\n        // Fetch HTML content\n        String htmlContent = fetchHtml(url);\n\n// Remove script and style elements\n        String noScriptStyle = removeElements(htmlContent, \"script\", \"style\");\n\n// Extract text (simple approach - remove HTML tags)\n        String text = removeHtmlTags(noScriptStyle);\n\n// Clean up text (break into lines, remove whitespace)\n        List<String> lines = Arrays.asList(text.split(\"\\n\"));\n        List<String> trimmedLines = lines.stream()\n                .map(String::trim)\n                .collect(toList());\n\n// Split on double spaces and flatten\n        List<String> chunks = trimmedLines.stream()\n                .flatMap(line -> Arrays.stream(line.split(\"  \"))\n                        .map(String::trim))\n                .collect(toList());\n\n// Filter empty chunks and join with newlines\n        return chunks.stream()\n                .filter(chunk -> !chunk.isEmpty())\n                .collect(joining(\"\\n\"));\n    }\n\n/**\n     * Fetches HTML content from a URL\n     */\n    private static String fetchHtml(String urlString) throws IOException {\n        try (InputStream inputStream = new URL(urlString).openStream()) {\n            StringBuilder content = new StringBuilder();\n            try (BufferedReader reader = new BufferedReader(\n                    new InputStreamReader(inputStream))) {\n                String line;\n                while ((line = reader.readLine()) != null) {\n                    content.append(line).append(\"\\n\");\n                }\n            }\n            return content.toString();\n        }\n    }\n\n/**\n     * Removes specified HTML elements and their content\n     */\n    private static String removeElements(String html, String... elementNames) {\n        String result = html;\n        for (String element : elementNames) {\n            // Pattern to match <element>...</element> and self-closing tags\n            String pattern = \"<\" + element + \"\\\\s*[^>]*>.*?</\" + element + \">|<\" + element + \"\\\\s*[^>]*/?>\";\n            result = Pattern.compile(pattern, Pattern.DOTALL).matcher(result).replaceAll(\"\");\n        }\n        return result;\n    }\n\n/**\n     * Removes all HTML tags from content\n     */\n    private static String removeHtmlTags(String html) {\n        // Replace <br> and <p> tags with newlines for better text formatting\n        String withLineBreaks = html.replaceAll(\"<br\\\\s*/?\\\\s*>|</?p\\\\s*[^>]*>\", \"\\n\");\n\n// Remove remaining HTML tags\n        String noTags = withLineBreaks.replaceAll(\"<[^>]*>\", \"\");\n\n// Decode HTML entities (simplified for common entities)\n        return decodeHtmlEntities(noTags);\n    }\n\n/**\n     * Simple HTML entity decoder for common entities\n     */\n    private static String decodeHtmlEntities(String text) {\n        return text\n                .replaceAll(\"&nbsp;\", \" \")\n                .replaceAll(\"&amp;\", \"&\")\n                .replaceAll(\"&lt;\", \"<\")\n                .replaceAll(\"&gt;\", \">\")\n                .replaceAll(\"&quot;\", \"\\\"\")\n                .replaceAll(\"&#39;\", \"'\")\n                .replaceAll(\"&hellip;\", \"...\")\n                .replaceAll(\"&mdash;\", \"â€”\");\n    }\n\nFirst request - establishing cache\nFirst response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 17, output_tokens: 700 }\n\nSecond request - same thinking parameters (cache hit expected)\n\nSecond response usage: { cache_creation_input_tokens: 0, cache_read_input_tokens: 1370, input_tokens: 303, output_tokens: 874 }\n\nThird request - different thinking budget (cache miss expected)\nThird response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 747, output_tokens: 619 }\n\ncontext window =\n  (current input tokens - previous thinking tokens) +\n  (thinking tokens + encrypted thinking tokens + text output tokens)\n\ncontext window =\n  (current input tokens + previous thinking tokens + tool use tokens) +\n  (thinking tokens + encrypted thinking tokens + text output tokens)\njson\n{\n  \"content\": [\n    {\n      \"type\": \"thinking\",\n      \"thinking\": \"Let me analyze this step by step...\",\n      \"signature\": \"WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL....\"\n    },\n    {\n      \"type\": \"redacted_thinking\",\n      \"data\": \"EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ...\"\n    },\n    {\n      \"type\": \"text\",\n      \"text\": \"Based on my analysis...\"\n    }\n  ]\n}\npython Python\nimport anthropic\n\nclient = anthropic.Anthropic()",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nHere is the output of the script (you may see slightly different numbers)",
      "language": "unknown"
    },
    {
      "code": "This example demonstrates that when caching is set up in the messages array, changing the thinking parameters (budget_tokens increased from 4000 to 8000) **invalidates the cache**. The third request shows no cache hit with `cache_creation_input_tokens=1370` and `cache_read_input_tokens=0`, proving that message-based caching is invalidated when thinking parameters change.\n\n</section>\n\n## Max tokens and context window size with extended thinking\n\nIn older Claude models (prior to Claude Sonnet 3.7), if the sum of prompt tokens and `max_tokens` exceeded the model's context window, the system would automatically adjust `max_tokens` to fit within the context limit. This meant you could set a large `max_tokens` value and the system would silently reduce it as needed.\n\nWith Claude 3.7 and 4 models, `max_tokens` (which includes your thinking budget when thinking is enabled) is enforced as a strict limit. The system will now return a validation error if prompt tokens + `max_tokens` exceeds the context window size.\n\n<Note>\nYou can read through our [guide on context windows](/docs/en/build-with-claude/context-windows) for a more thorough deep dive.\n</Note>\n\n### The context window with extended thinking\n\nWhen calculating context window usage with thinking enabled, there are some considerations to be aware of:\n\n- Thinking blocks from previous turns are stripped and not counted towards your context window\n- Current turn thinking counts towards your `max_tokens` limit for that turn\n\nThe diagram below demonstrates the specialized token management when extended thinking is enabled:\n\n![Context window diagram with extended thinking](/docs/images/context-window-thinking.svg)\n\nThe effective context window is calculated as:",
      "language": "unknown"
    },
    {
      "code": "We recommend using the [token counting API](/docs/en/build-with-claude/token-counting) to get accurate token counts for your specific use case, especially when working with multi-turn conversations that include thinking.\n\n### The context window with extended thinking and tool use\n\nWhen using extended thinking with tool use, thinking blocks must be explicitly preserved and returned with the tool results.\n\nThe effective context window calculation for extended thinking with tool use becomes:",
      "language": "unknown"
    },
    {
      "code": "The diagram below illustrates token management for extended thinking with tool use:\n\n![Context window diagram with extended thinking and tool use](/docs/images/context-window-thinking-tools.svg)\n\n### Managing tokens with extended thinking\n\nGiven the context window and `max_tokens` behavior with extended thinking Claude 3.7 and 4 models, you may need to:\n\n- More actively monitor and manage your token usage\n- Adjust `max_tokens` values as your prompt length changes\n- Potentially use the [token counting endpoints](/docs/en/build-with-claude/token-counting) more frequently\n- Be aware that previous thinking blocks don't accumulate in your context window\n\nThis change has been made to provide more predictable and transparent behavior, especially as maximum token limits have increased significantly.\n\n## Thinking encryption\n\nFull thinking content is encrypted and returned in the `signature` field. This field is used to verify that thinking blocks were generated by Claude when passed back to the API. \n\n<Note>\nIt is only strictly necessary to send back thinking blocks when using [tools with extended thinking](#extended-thinking-with-tool-use). Otherwise you can omit thinking blocks from previous turns, or let the API strip them for you if you pass them back. \n\nIf sending back thinking blocks, we recommend passing everything back as you received it for consistency and to avoid potential issues.\n</Note>\n\nHere are some important considerations on thinking encryption:\n- When [streaming responses](#streaming-thinking), the signature is added via a `signature_delta` inside a `content_block_delta` event just before the `content_block_stop` event.\n- `signature` values are significantly longer in Claude 4 models than in previous models.\n- The `signature` field is an opaque field and should not be interpreted or parsed - it exists solely for verification purposes.\n- `signature` values are compatible across platforms (Claude APIs, [Amazon Bedrock](/docs/en/build-with-claude/claude-on-amazon-bedrock), and [Vertex AI](/docs/en/build-with-claude/claude-on-vertex-ai)). Values generated on one platform will be compatible with another.\n\n### Thinking redaction\n\nOccasionally Claude's internal reasoning will be flagged by our safety systems. When this occurs, we encrypt some or all of the `thinking` block and return it to you as a `redacted_thinking` block. `redacted_thinking` blocks are decrypted when passed back to the API, allowing Claude to continue its response without losing context.\n\nWhen building customer-facing applications that use extended thinking:\n\n- Be aware that redacted thinking blocks contain encrypted content that isn't human-readable\n- Consider providing a simple explanation like: \"Some of Claude's internal reasoning has been automatically encrypted for safety reasons. This doesn't affect the quality of responses.\"\n- If showing thinking blocks to users, you can filter out redacted blocks while preserving normal thinking blocks\n- Be transparent that using extended thinking features may occasionally result in some reasoning being encrypted\n- Implement appropriate error handling to gracefully manage redacted thinking without breaking your UI\n\nHere's an example showing both normal and redacted thinking blocks:",
      "language": "unknown"
    },
    {
      "code": "<Note>\nSeeing redacted thinking blocks in your output is expected behavior. The model can still use this redacted reasoning to inform its responses while maintaining safety guardrails.\n\nIf you need to test redacted thinking handling in your application, you can use this special test string as your prompt: `ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`\n</Note>\n\nWhen passing `thinking` and `redacted_thinking` blocks back to the API in a multi-turn conversation, you must include the complete unmodified block back to the API for the last assistant turn. This is critical for maintaining the model's reasoning flow. We suggest always passing back all thinking blocks to the API. For more details, see the [Preserving thinking blocks](#preserving-thinking-blocks) section above.\n\n<section title=\"Example: Working with redacted thinking blocks\">\n\nThis example demonstrates how to handle `redacted_thinking` blocks that may appear in responses when Claude's internal reasoning contains content flagged by safety systems:\n\n<CodeGroup>",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Max tokens and context window size with extended thinking",
      "id": "max-tokens-and-context-window-size-with-extended-thinking"
    },
    {
      "level": "h3",
      "text": "The context window with extended thinking",
      "id": "the-context-window-with-extended-thinking"
    },
    {
      "level": "h3",
      "text": "The context window with extended thinking and tool use",
      "id": "the-context-window-with-extended-thinking-and-tool-use"
    },
    {
      "level": "h3",
      "text": "Managing tokens with extended thinking",
      "id": "managing-tokens-with-extended-thinking"
    },
    {
      "level": "h2",
      "text": "Thinking encryption",
      "id": "thinking-encryption"
    },
    {
      "level": "h3",
      "text": "Thinking redaction",
      "id": "thinking-redaction"
    }
  ],
  "url": "llms-txt#third-request---different-thinking-budget-(cache-miss-expected)",
  "links": []
}