{
  "title": "No new thinking blocks will be generated in the response",
  "content": "continuation = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=16000,\n    thinking={\n        \"type\": \"enabled\",\n        \"budget_tokens\": 10000\n    },\n    tools=[weather_tool],\n    messages=[\n        {\"role\": \"user\", \"content\": \"What's the weather in Paris?\"},\n        # notice that the thinking_block is passed in as well as the tool_use_block\n        # if this is not passed in, an error is raised\n        {\"role\": \"assistant\", \"content\": [thinking_block, tool_use_block]},\n        {\"role\": \"user\", \"content\": [{\n            \"type\": \"tool_result\",\n            \"tool_use_id\": tool_use_block.id,\n            \"content\": f\"Current temperature: {weather_data['temperature']}°F\"\n        }]}\n    ]\n)\ntypescript TypeScript\n// Extract thinking block and tool use block\nconst thinkingBlock = response.content.find(block =>\n  block.type === 'thinking');\nconst toolUseBlock = response.content.find(block =>\n  block.type === 'tool_use');\n\n// Call your actual weather API, here is where your actual API call would go\n// let's pretend this is what we get back\nconst weatherData = { temperature: 88 };\n\n// Second request - Include thinking block and tool result\n// No new thinking blocks will be generated in the response\nconst continuation = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 16000,\n  thinking: {\n    type: \"enabled\",\n    budget_tokens: 10000\n  },\n  tools: [weatherTool],\n  messages: [\n    { role: \"user\", content: \"What's the weather in Paris?\" },\n    // notice that the thinkingBlock is passed in as well as the toolUseBlock\n    // if this is not passed in, an error is raised\n    { role: \"assistant\", content: [thinkingBlock, toolUseBlock] },\n    { role: \"user\", content: [{\n      type: \"tool_result\",\n      tool_use_id: toolUseBlock.id,\n      content: `Current temperature: ${weatherData.temperature}°F`\n    }]}\n  ]\n});\njava Java\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Optional;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.core.JsonValue;\nimport com.anthropic.models.beta.messages.*;\nimport com.anthropic.models.beta.messages.BetaTool.InputSchema;\nimport com.anthropic.models.messages.Model;\n\npublic class ThinkingToolsResultExample {\n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\nInputSchema schema = InputSchema.builder()\n                .properties(JsonValue.from(Map.of(\n                        \"location\", Map.of(\"type\", \"string\")\n                )))\n                .putAdditionalProperty(\"required\", JsonValue.from(List.of(\"location\")))\n                .build();\n\nBetaTool weatherTool = BetaTool.builder()\n                .name(\"get_weather\")\n                .description(\"Get current weather for a location\")\n                .inputSchema(schema)\n                .build();\n\nBetaMessage response = client.beta().messages().create(\n                MessageCreateParams.builder()\n                        .model(Model.CLAUDE_OPUS_4_0)\n                        .maxTokens(16000)\n                        .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                        .addTool(weatherTool)\n                        .addUserMessage(\"What's the weather in Paris?\")\n                        .build()\n        );\n\n// Extract thinking block and tool use block\n        Optional<BetaThinkingBlock> thinkingBlockOpt = response.content().stream()\n                .filter(BetaContentBlock::isThinking)\n                .map(BetaContentBlock::asThinking)\n                .findFirst();\n\nOptional<BetaToolUseBlock> toolUseBlockOpt = response.content().stream()\n                .filter(BetaContentBlock::isToolUse)\n                .map(BetaContentBlock::asToolUse)\n                .findFirst();\n\nif (thinkingBlockOpt.isPresent() && toolUseBlockOpt.isPresent()) {\n            BetaThinkingBlock thinkingBlock = thinkingBlockOpt.get();\n            BetaToolUseBlock toolUseBlock = toolUseBlockOpt.get();\n\n// Call your actual weather API, here is where your actual API call would go\n            // let's pretend this is what we get back\n            Map<String, Object> weatherData = Map.of(\"temperature\", 88);\n\n// Second request - Include thinking block and tool result\n            // No new thinking blocks will be generated in the response\n            BetaMessage continuation = client.beta().messages().create(\n                    MessageCreateParams.builder()\n                            .model(Model.CLAUDE_OPUS_4_0)\n                            .maxTokens(16000)\n                            .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())\n                            .addTool(weatherTool)\n                            .addUserMessage(\"What's the weather in Paris?\")\n                            .addAssistantMessageOfBetaContentBlockParams(\n                                    // notice that the thinkingBlock is passed in as well as the toolUseBlock\n                                    // if this is not passed in, an error is raised\n                                    List.of(\n                                            BetaContentBlockParam.ofThinking(thinkingBlock.toParam()),\n                                            BetaContentBlockParam.ofToolUse(toolUseBlock.toParam())\n                                    )\n                            )\n                            .addUserMessageOfBetaContentBlockParams(List.of(\n                                    BetaContentBlockParam.ofToolResult(\n                                            BetaToolResultBlockParam.builder()\n                                                    .toolUseId(toolUseBlock.id())\n                                                    .content(String.format(\"Current temperature: %d°F\", (Integer)weatherData.get(\"temperature\")))\n                                                    .build()\n                                    )\n                            ))\n                            .build()\n            );\n\nSystem.out.println(continuation);\n        }\n    }\n}\njson\n{\n    \"content\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"Currently in Paris, the temperature is 88°F (31°C)\"\n        }\n    ]\n}\n\nUser: \"What's the total revenue if we sold 150 units at $50 each,\n       and how does this compare to our average monthly revenue?\"\n\nTurn 1: [thinking] \"I need to calculate 150 * $50, then check the database...\"\n        [tool_use: calculator] { \"expression\": \"150 * 50\" }\n  ↓ tool result: \"7500\"\n\nTurn 2: [tool_use: database_query] { \"query\": \"SELECT AVG(revenue)...\" }\n        ↑ no thinking block\n  ↓ tool result: \"5200\"\n\nTurn 3: [text] \"The total revenue is $7,500, which is 44% above your\n        average monthly revenue of $5,200.\"\n        ↑ no thinking block\n\nUser: \"What's the total revenue if we sold 150 units at $50 each,\n       and how does this compare to our average monthly revenue?\"\n\nTurn 1: [thinking] \"I need to calculate 150 * $50 first...\"\n        [tool_use: calculator] { \"expression\": \"150 * 50\" }\n  ↓ tool result: \"7500\"\n\nTurn 2: [thinking] \"Got $7,500. Now I should query the database to compare...\"\n        [tool_use: database_query] { \"query\": \"SELECT AVG(revenue)...\" }\n        ↑ thinking after receiving calculator result\n  ↓ tool result: \"5200\"\n\nTurn 3: [thinking] \"$7,500 vs $5,200 average - that's a 44% increase...\"\n        [text] \"The total revenue is $7,500, which is 44% above your\n        average monthly revenue of $5,200.\"\n        ↑ thinking before final answer\n\nUser: \"What's the weather in Paris?\"\n\n[thinking_block_1] + [tool_use block 1]\n\nUser: [\"What's the weather in Paris?\"], \nAssistant: [thinking_block_1] + [tool_use block 1], \nUser: [tool_result_1, cache=True]\n\n[thinking_block_2] + [text block 2]\n\nUser: [\"What's the weather in Paris?\"],\nAssistant: [thinking_block_1] + [tool_use block 1],\nUser: [tool_result_1, cache=True],\nAssistant: [thinking_block_2] + [text block 2],\nUser: [Text response, cache=True]\n\nUser: [\"What's the weather in Paris?\"],\nAssistant: [tool_use block 1],\nUser: [tool_result_1, cache=True],\nAssistant: [text block 2],\nUser: [Text response, cache=True]\npython Python\nfrom anthropic import Anthropic\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef fetch_article_content(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n\n# Remove script and style elements\n    for script in soup([\"script\", \"style\"]):\n        script.decompose()\n\n# Get text\n    text = soup.get_text()\n\n# Break into lines and remove leading and trailing space on each\n    lines = (line.strip() for line in text.splitlines())\n    # Break multi-headlines into a line each\n    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n    # Drop blank lines\n    text = '\\n'.join(chunk for chunk in chunks if chunk)",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nThe API response will now **only** include text",
      "language": "unknown"
    },
    {
      "code": "</section>\n\n### Preserving thinking blocks\n\nDuring tool use, you must pass `thinking` blocks back to the API, and you must include the complete unmodified block back to the API. This is critical for maintaining the model's reasoning flow and conversation integrity.\n\n<Tip>\nWhile you can omit `thinking` blocks from prior `assistant` role turns, we suggest always passing back all thinking blocks to the API for any multi-turn conversation. The API will:\n- Automatically filter the provided thinking blocks\n- Use the relevant thinking blocks necessary to preserve the model's reasoning\n- Only bill for the input tokens for the blocks shown to Claude\n</Tip>\n\n<Note>\nWhen toggling thinking modes during a conversation, remember that the entire assistant turn (including tool use loops) must operate in a single thinking mode. For more details, see [Toggling thinking modes in conversations](#toggling-thinking-modes-in-conversations).\n</Note>\n\nWhen Claude invokes tools, it is pausing its construction of a response to await external information. When tool results are returned, Claude will continue building that existing response. This necessitates preserving thinking blocks during tool use, for a couple of reasons:\n\n1. **Reasoning continuity**: The thinking blocks capture Claude's step-by-step reasoning that led to tool requests. When you post tool results, including the original thinking ensures Claude can continue its reasoning from where it left off.\n\n2. **Context maintenance**: While tool results appear as user messages in the API structure, they're part of a continuous reasoning flow. Preserving thinking blocks maintains this conceptual flow across multiple API calls. For more information on context management, see our [guide on context windows](/docs/en/build-with-claude/context-windows).\n\n**Important**: When providing `thinking` blocks, the entire sequence of consecutive `thinking` blocks must match the outputs generated by the model during the original request; you cannot rearrange or modify the sequence of these blocks.\n\n### Interleaved thinking\n\nExtended thinking with tool use in Claude 4 models supports interleaved thinking, which enables Claude to think between tool calls and make more sophisticated reasoning after receiving tool results.\n\nWith interleaved thinking, Claude can:\n- Reason about the results of a tool call before deciding what to do next\n- Chain multiple tool calls with reasoning steps in between\n- Make more nuanced decisions based on intermediate results\n\nTo enable interleaved thinking, add [the beta header](/docs/en/api/beta-headers) `interleaved-thinking-2025-05-14` to your API request.\n\nHere are some important considerations for interleaved thinking:\n- With interleaved thinking, the `budget_tokens` can exceed the `max_tokens` parameter, as it represents the total budget across all thinking blocks within one assistant turn.\n- Interleaved thinking is only supported for [tools used via the Messages API](/docs/en/agents-and-tools/tool-use/overview).\n- Interleaved thinking is supported for Claude 4 models only, with the beta header `interleaved-thinking-2025-05-14`.\n- Direct calls to the Claude API allow you to pass `interleaved-thinking-2025-05-14` in requests to any model, with no effect.\n- On 3rd-party platforms (e.g., [Amazon Bedrock](/docs/en/build-with-claude/claude-on-amazon-bedrock) and [Vertex AI](/docs/en/build-with-claude/claude-on-vertex-ai)), if you pass `interleaved-thinking-2025-05-14` to any model aside from Claude Opus 4.5, Claude Opus 4.1, Opus 4, or Sonnet 4, your request will fail.\n\n<section title=\"Tool use without interleaved thinking\">\n\nWithout interleaved thinking, Claude thinks once at the start of the assistant turn. Subsequent responses after tool results continue without new thinking blocks.",
      "language": "unknown"
    },
    {
      "code": "</section>\n\n<section title=\"Tool use with interleaved thinking\">\n\nWith interleaved thinking enabled, Claude can think after receiving each tool result, allowing it to reason about intermediate results before continuing.",
      "language": "unknown"
    },
    {
      "code": "</section>\n\n## Extended thinking with prompt caching\n\n[Prompt caching](/docs/en/build-with-claude/prompt-caching) with thinking has several important considerations:\n\n<Tip>\nExtended thinking tasks often take longer than 5 minutes to complete. Consider using the [1-hour cache duration](/docs/en/build-with-claude/prompt-caching#1-hour-cache-duration) to maintain cache hits across longer thinking sessions and multi-step workflows.\n</Tip>\n\n**Thinking block context removal**\n- Thinking blocks from previous turns are removed from context, which can affect cache breakpoints\n- When continuing conversations with tool use, thinking blocks are cached and count as input tokens when read from cache\n- This creates a tradeoff: while thinking blocks don't consume context window space visually, they still count toward your input token usage when cached\n- If thinking becomes disabled, requests will fail if you pass thinking content in the current tool use turn. In other contexts, thinking content passed to the API is simply ignored\n\n**Cache invalidation patterns**\n- Changes to thinking parameters (enabled/disabled or budget allocation) invalidate message cache breakpoints\n- [Interleaved thinking](#interleaved-thinking) amplifies cache invalidation, as thinking blocks can occur between multiple [tool calls](#extended-thinking-with-tool-use)\n- System prompts and tools remain cached despite thinking parameter changes or block removal\n\n<Note>\nWhile thinking blocks are removed for caching and context calculations, they must be preserved when continuing conversations with [tool use](#extended-thinking-with-tool-use), especially with [interleaved thinking](#interleaved-thinking).\n</Note>\n\n### Understanding thinking block caching behavior\n\nWhen using extended thinking with tool use, thinking blocks exhibit specific caching behavior that affects token counting:\n\n**How it works:**\n\n1. Caching only occurs when you make a subsequent request that includes tool results\n2. When the subsequent request is made, the previous conversation history (including thinking blocks) can be cached\n3. These cached thinking blocks count as input tokens in your usage metrics when read from the cache\n4. When a non-tool-result user block is included, all previous thinking blocks are ignored and stripped from context\n\n**Detailed example flow:**\n\n**Request 1:**",
      "language": "unknown"
    },
    {
      "code": "**Response 1:**",
      "language": "unknown"
    },
    {
      "code": "**Request 2:**",
      "language": "unknown"
    },
    {
      "code": "**Response 2:**",
      "language": "unknown"
    },
    {
      "code": "Request 2 writes a cache of the request content (not the response). The cache includes the original user message, the first thinking block, tool use block, and the tool result.\n\n**Request 3:**",
      "language": "unknown"
    },
    {
      "code": "For Claude Opus 4.5 and later, all previous thinking blocks are kept by default. For older models, because a non-tool-result user block was included, all previous thinking blocks are ignored. This request will be processed the same as:",
      "language": "unknown"
    },
    {
      "code": "**Key points:**\n- This caching behavior happens automatically, even without explicit `cache_control` markers\n- This behavior is consistent whether using regular thinking or interleaved thinking\n\n<section title=\"System prompt caching (preserved when thinking changes)\">\n\n<CodeGroup>",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Preserving thinking blocks",
      "id": "preserving-thinking-blocks"
    },
    {
      "level": "h3",
      "text": "Interleaved thinking",
      "id": "interleaved-thinking"
    },
    {
      "level": "h2",
      "text": "Extended thinking with prompt caching",
      "id": "extended-thinking-with-prompt-caching"
    },
    {
      "level": "h3",
      "text": "Understanding thinking block caching behavior",
      "id": "understanding-thinking-block-caching-behavior"
    }
  ],
  "url": "llms-txt#no-new-thinking-blocks-will-be-generated-in-the-response",
  "links": []
}