{
  "title": "Stream results file in memory-efficient chunks, processing one at a time",
  "content": "for result in client.messages.batches.results(\n    \"msgbatch_01HkcTjaV5uDC8jWR4ZsDV8d\",\n):\n    match result.result.type:\n        case \"succeeded\":\n            print(f\"Success! {result.custom_id}\")\n        case \"errored\":\n            if result.result.error.type == \"invalid_request\":\n                # Request body must be fixed before re-sending request\n                print(f\"Validation error {result.custom_id}\")\n            else:\n                # Request can be retried directly\n                print(f\"Server error {result.custom_id}\")\n        case \"expired\":\n            print(f\"Request expired {result.custom_id}\")\ntypescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst anthropic = new Anthropic();\n\n// Stream results file in memory-efficient chunks, processing one at a time\nfor await (const result of await anthropic.messages.batches.results(\n    \"msgbatch_01HkcTjaV5uDC8jWR4ZsDV8d\"\n)) {\n  switch (result.result.type) {\n    case 'succeeded':\n      console.log(`Success! ${result.custom_id}`);\n      break;\n    case 'errored':\n      if (result.result.error.type == \"invalid_request\") {\n        // Request body must be fixed before re-sending request\n        console.log(`Validation error: ${result.custom_id}`);\n      } else {\n        // Request can be retried directly\n        console.log(`Server error: ${result.custom_id}`);\n      }\n      break;\n    case 'expired':\n      console.log(`Request expired: ${result.custom_id}`);\n      break;\n  }\n}\njava Java\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.core.http.StreamResponse;\nimport com.anthropic.models.messages.batches.MessageBatchIndividualResponse;\nimport com.anthropic.models.messages.batches.BatchResultsParams;\n\npublic class BatchResultsExample {\n\npublic static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n// Stream results file in memory-efficient chunks, processing one at a time\n        try (StreamResponse<MessageBatchIndividualResponse> streamResponse = client.messages()\n                .batches()\n                .resultsStreaming(\n                        BatchResultsParams.builder()\n                                .messageBatchId(\"msgbatch_01HkcTjaV5uDC8jWR4ZsDV8d\")\n                                .build())) {\n\nstreamResponse.stream().forEach(result -> {\n                if (result.result().isSucceeded()) {\n                    System.out.println(\"Success! \" + result.customId());\n                } else if (result.result().isErrored()) {\n                    if (result.result().asErrored().error().error().isInvalidRequestError()) {\n                        // Request body must be fixed before re-sending request\n                        System.out.println(\"Validation error: \" + result.customId());\n                    } else {\n                        // Request can be retried directly\n                        System.out.println(\"Server error: \" + result.customId());\n                    }\n                } else if (result.result().isExpired()) {\n                    System.out.println(\"Request expired: \" + result.customId());\n                }\n            });\n        }\n    }\n}\njson .jsonl file\n{\"custom_id\":\"my-second-request\",\"result\":{\"type\":\"succeeded\",\"message\":{\"id\":\"msg_014VwiXbi91y3JMjcpyGBHX5\",\"type\":\"message\",\"role\":\"assistant\",\"model\":\"claude-sonnet-4-5-20250929\",\"content\":[{\"type\":\"text\",\"text\":\"Hello again! It's nice to see you. How can I assist you today? Is there anything specific you'd like to chat about or any questions you have?\"}],\"stop_reason\":\"end_turn\",\"stop_sequence\":null,\"usage\":{\"input_tokens\":11,\"output_tokens\":36}}}}\n{\"custom_id\":\"my-first-request\",\"result\":{\"type\":\"succeeded\",\"message\":{\"id\":\"msg_01FqfsLoHwgeFbguDgpz48m7\",\"type\":\"message\",\"role\":\"assistant\",\"model\":\"claude-sonnet-4-5-20250929\",\"content\":[{\"type\":\"text\",\"text\":\"Hello! How can I assist you today? Feel free to ask me any questions or let me know if there's anything you'd like to chat about.\"}],\"stop_reason\":\"end_turn\",\"stop_sequence\":null,\"usage\":{\"input_tokens\":10,\"output_tokens\":34}}}}\npython Python\nimport anthropic\n\nclient = anthropic.Anthropic()\n\nmessage_batch = client.messages.batches.cancel(\n    MESSAGE_BATCH_ID,\n)\nprint(message_batch)\ntypescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst anthropic = new Anthropic();\n\nconst messageBatch = await anthropic.messages.batches.cancel(\n    MESSAGE_BATCH_ID\n);\nconsole.log(messageBatch);\nbash Shell\n#!/bin/sh\ncurl --request POST https://api.anthropic.com/v1/messages/batches/$MESSAGE_BATCH_ID/cancel \\\n    --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n    --header \"anthropic-version: 2023-06-01\"\njava Java\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.models.messages.batches.*;\n\npublic class BatchCancelExample {\n    public static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\nMessageBatch messageBatch = client.messages().batches().cancel(\n                BatchCancelParams.builder()\n                        .messageBatchId(MESSAGE_BATCH_ID)\n                        .build()\n        );\n        System.out.println(messageBatch);\n    }\n}\njson JSON\n{\n  \"id\": \"msgbatch_013Zva2CMHLNnXjNJJKqJ2EF\",\n  \"type\": \"message_batch\",\n  \"processing_status\": \"canceling\",\n  \"request_counts\": {\n    \"processing\": 2,\n    \"succeeded\": 0,\n    \"errored\": 0,\n    \"canceled\": 0,\n    \"expired\": 0\n  },\n  \"ended_at\": null,\n  \"created_at\": \"2024-09-24T18:37:24.100435Z\",\n  \"expires_at\": \"2024-09-25T18:37:24.100435Z\",\n  \"cancel_initiated_at\": \"2024-09-24T18:39:03.114875Z\",\n  \"results_url\": null\n}\nbash Shell\ncurl https://api.anthropic.com/v1/messages/batches \\\n     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n     --header \"anthropic-version: 2023-06-01\" \\\n     --header \"content-type: application/json\" \\\n     --data \\\n'{\n    \"requests\": [\n        {\n            \"custom_id\": \"my-first-request\",\n            \"params\": {\n                \"model\": \"claude-sonnet-4-5\",\n                \"max_tokens\": 1024,\n                \"system\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\\n\"\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"<the entire contents of Pride and Prejudice>\",\n                        \"cache_control\": {\"type\": \"ephemeral\"}\n                    }\n                ],\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"Analyze the major themes in Pride and Prejudice.\"}\n                ]\n            }\n        },\n        {\n            \"custom_id\": \"my-second-request\",\n            \"params\": {\n                \"model\": \"claude-sonnet-4-5\",\n                \"max_tokens\": 1024,\n                \"system\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\\n\"\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"<the entire contents of Pride and Prejudice>\",\n                        \"cache_control\": {\"type\": \"ephemeral\"}\n                    }\n                ],\n                \"messages\": [\n                    {\"role\": \"user\", \"content\": \"Write a summary of Pride and Prejudice.\"}\n                ]\n            }\n        }\n    ]\n}'\npython Python\nimport anthropic\nfrom anthropic.types.message_create_params import MessageCreateParamsNonStreaming\nfrom anthropic.types.messages.batch_create_params import Request\n\nclient = anthropic.Anthropic()\n\nmessage_batch = client.messages.batches.create(\n    requests=[\n        Request(\n            custom_id=\"my-first-request\",\n            params=MessageCreateParamsNonStreaming(\n                model=\"claude-sonnet-4-5\",\n                max_tokens=1024,\n                system=[\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\\n\"\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"<the entire contents of Pride and Prejudice>\",\n                        \"cache_control\": {\"type\": \"ephemeral\"}\n                    }\n                ],\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": \"Analyze the major themes in Pride and Prejudice.\"\n                }]\n            )\n        ),\n        Request(\n            custom_id=\"my-second-request\",\n            params=MessageCreateParamsNonStreaming(\n                model=\"claude-sonnet-4-5\",\n                max_tokens=1024,\n                system=[\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\\n\"\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"<the entire contents of Pride and Prejudice>\",\n                        \"cache_control\": {\"type\": \"ephemeral\"}\n                    }\n                ],\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": \"Write a summary of Pride and Prejudice.\"\n                }]\n            )\n        )\n    ]\n)\ntypescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst anthropic = new Anthropic();\n\nconst messageBatch = await anthropic.messages.batches.create({\n  requests: [{\n    custom_id: \"my-first-request\",\n    params: {\n      model: \"claude-sonnet-4-5\",\n      max_tokens: 1024,\n      system: [\n        {\n          type: \"text\",\n          text: \"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\\n\"\n        },\n        {\n          type: \"text\",\n          text: \"<the entire contents of Pride and Prejudice>\",\n          cache_control: {type: \"ephemeral\"}\n        }\n      ],\n      messages: [\n        {\"role\": \"user\", \"content\": \"Analyze the major themes in Pride and Prejudice.\"}\n      ]\n    }\n  }, {\n    custom_id: \"my-second-request\",\n    params: {\n      model: \"claude-sonnet-4-5\",\n      max_tokens: 1024,\n      system: [\n        {\n          type: \"text\",\n          text: \"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\\n\"\n        },\n        {\n          type: \"text\",\n          text: \"<the entire contents of Pride and Prejudice>\",\n          cache_control: {type: \"ephemeral\"}\n        }\n      ],\n      messages: [\n        {\"role\": \"user\", \"content\": \"Write a summary of Pride and Prejudice.\"}\n      ]\n    }\n  }]\n});\njava Java\nimport java.util.List;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.models.messages.CacheControlEphemeral;\nimport com.anthropic.models.messages.Model;\nimport com.anthropic.models.messages.TextBlockParam;\nimport com.anthropic.models.messages.batches.*;\n\npublic class BatchExample {\n\npublic static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\nBatchCreateParams createParams = BatchCreateParams.builder()\n                .addRequest(BatchCreateParams.Request.builder()\n                        .customId(\"my-first-request\")\n                        .params(BatchCreateParams.Request.Params.builder()\n                                .model(Model.CLAUDE_OPUS_4_0)\n                                .maxTokens(1024)\n                                .systemOfTextBlockParams(List.of(\n                                        TextBlockParam.builder()\n                                                .text(\"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\\n\")\n                                                .build(),\n                                        TextBlockParam.builder()\n                                                .text(\"<the entire contents of Pride and Prejudice>\")\n                                                .cacheControl(CacheControlEphemeral.builder().build())\n                                                .build()\n                                ))\n                                .addUserMessage(\"Analyze the major themes in Pride and Prejudice.\")\n                                .build())\n                        .build())\n                .addRequest(BatchCreateParams.Request.builder()\n                        .customId(\"my-second-request\")\n                        .params(BatchCreateParams.Request.Params.builder()\n                                .model(Model.CLAUDE_OPUS_4_0)\n                                .maxTokens(1024)\n                                .systemOfTextBlockParams(List.of(\n                                        TextBlockParam.builder()\n                                                .text(\"You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\\n\")\n                                                .build(),\n                                        TextBlockParam.builder()\n                                                .text(\"<the entire contents of Pride and Prejudice>\")\n                                                .cacheControl(CacheControlEphemeral.builder().build())\n                                                .build()\n                                ))\n                                .addUserMessage(\"Write a summary of Pride and Prejudice.\")\n                                .build())\n                        .build())\n                .build();\n\nMessageBatch messageBatch = client.messages().batches().create(createParams);\n    }\n}\n```\n\nIn this example, both requests in the batch include identical system messages and the full text of Pride and Prejudice marked with `cache_control` to increase the likelihood of cache hits.\n\n### Best practices for effective batching\n\nTo get the most out of the Batches API:\n\n- Monitor batch processing status regularly and implement appropriate retry logic for failed requests.\n- Use meaningful `custom_id` values to easily match results with requests, since order is not guaranteed.\n- Consider breaking very large datasets into multiple batches for better manageability.\n- Dry run a single request shape with the Messages API to avoid validation errors.\n\n### Troubleshooting common issues\n\nIf experiencing unexpected behavior:\n\n- Verify that the total batch request size doesn't exceed 256 MB. If the request size is too large, you may get a 413 `request_too_large` error.\n- Check that you're using [supported models](#supported-models) for all requests in the batch.\n- Ensure each request in the batch has a unique `custom_id`.\n- Ensure that it has been less than 29 days since batch `created_at` (not processing `ended_at`) time. If over 29 days have passed, results will no longer be viewable.\n- Confirm that the batch has not been canceled.\n\nNote that the failure of one request in a batch does not affect the processing of other requests.\n\n---\n## Batch storage and privacy\n\n- **Workspace isolation**: Batches are isolated within the Workspace they are created in. They can only be accessed by API keys associated with that Workspace, or users with permission to view Workspace batches in the Console.\n\n- **Result availability**: Batch results are available for 29 days after the batch is created, allowing ample time for retrieval and processing.\n\n<section title=\"How long does it take for a batch to process?\">\n\nBatches may take up to 24 hours for processing, but many will finish sooner. Actual processing time depends on the size of the batch, current demand, and your request volume. It is possible for a batch to expire and not complete within 24 hours.\n  \n</section>\n\n<section title=\"Is the Batches API available for all models?\">\n\nSee [above](#supported-models) for the list of supported models.\n  \n</section>\n\n<section title=\"Can I use the Message Batches API with other API features?\">\n\nYes, the Message Batches API supports all features available in the Messages API, including beta features. However, streaming is not supported for batch requests.\n  \n</section>\n\n<section title=\"How does the Message Batches API affect pricing?\">\n\nThe Message Batches API offers a 50% discount on all usage compared to standard API prices. This applies to input tokens, output tokens, and any special tokens. For more on pricing, visit our [pricing page](https://claude.com/pricing#anthropic-api).\n  \n</section>\n\n<section title=\"Can I update a batch after it's been submitted?\">\n\nNo, once a batch has been submitted, it cannot be modified. If you need to make changes, you should cancel the current batch and submit a new one. Note that cancellation may not take immediate effect.\n  \n</section>\n\n<section title=\"Are there Message Batches API rate limits and do they interact with the Messages API rate limits?\">\n\nThe Message Batches API has HTTP requests-based rate limits in addition to limits on the number of requests in need of processing. See [Message Batches API rate limits](/docs/en/api/rate-limits#message-batches-api). Usage of the Batches API does not affect rate limits in the Messages API.\n  \n</section>\n\n<section title=\"How do I handle errors in my batch requests?\">\n\nWhen you retrieve the results, each request will have a `result` field indicating whether it `succeeded`, `errored`, was `canceled`, or `expired`. For `errored` results, additional error information will be provided. View the error response object in the [API reference](/docs/en/api/creating-message-batches).\n  \n</section>\n\n<section title=\"How does the Message Batches API handle privacy and data separation?\">\n\nThe Message Batches API is designed with strong privacy and data separation measures:\n\n1. Batches and their results are isolated within the Workspace in which they were created. This means they can only be accessed by API keys from that same Workspace.\n    2. Each request within a batch is processed independently, with no data leakage between requests.\n    3. Results are only available for a limited time (29 days), and follow our [data retention policy](https://support.claude.com/en/articles/7996866-how-long-do-you-store-personal-data).\n    4. Downloading batch results in the Console can be disabled on the organization-level or on a per-workspace basis.\n  \n</section>\n\n<section title=\"Can I use prompt caching in the Message Batches API?\">\n\nYes, it is possible to use prompt caching with Message Batches API. However, because asynchronous batch requests can be processed concurrently and in any order, cache hits are provided on a best-effort basis.\n  \n</section>",
  "code_samples": [
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nThe results will be in `.jsonl` format, where each line is a valid JSON object representing the result of a single request in the Message Batch. For each streamed result, you can do something different depending on its `custom_id` and result type. Here is an example set of results:",
      "language": "unknown"
    },
    {
      "code": "If your result has an error, its `result.error` will be set to our standard [error shape](/docs/en/api/errors#error-shapes).\n\n<Tip>\n  **Batch results may not match input order**\n\nBatch results can be returned in any order, and may not match the ordering of requests when the batch was created. In the above example, the result for the second batch request is returned before the first. To correctly match results with their corresponding requests, always use the `custom_id` field.\n</Tip>\n\n### Canceling a Message Batch\n\nYou can cancel a Message Batch that is currently processing using the [cancel endpoint](/docs/en/api/canceling-message-batches). Immediately after cancellation, a batch's `processing_status` will be `canceling`. You can use the same polling technique described above to wait until cancellation is finalized. Canceled batches end up with a status of `ended` and may contain partial results for requests that were processed before cancellation.\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nThe response will show the batch in a `canceling` state:",
      "language": "unknown"
    },
    {
      "code": "### Using prompt caching with Message Batches\n\nThe Message Batches API supports prompt caching, allowing you to potentially reduce costs and processing time for batch requests. The pricing discounts from prompt caching and Message Batches can stack, providing even greater cost savings when both features are used together. However, since batch requests are processed asynchronously and concurrently, cache hits are provided on a best-effort basis. Users typically experience cache hit rates ranging from 30% to 98%, depending on their traffic patterns.\n\nTo maximize the likelihood of cache hits in your batch requests:\n\n1. Include identical `cache_control` blocks in every Message request within your batch\n2. Maintain a steady stream of requests to prevent cache entries from expiring after their 5-minute lifetime\n3. Structure your requests to share as much cached content as possible\n\nExample of implementing prompt caching in a batch:\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Canceling a Message Batch",
      "id": "canceling-a-message-batch"
    },
    {
      "level": "h3",
      "text": "Using prompt caching with Message Batches",
      "id": "using-prompt-caching-with-message-batches"
    },
    {
      "level": "h3",
      "text": "Best practices for effective batching",
      "id": "best-practices-for-effective-batching"
    },
    {
      "level": "h3",
      "text": "Troubleshooting common issues",
      "id": "troubleshooting-common-issues"
    },
    {
      "level": "h2",
      "text": "Batch storage and privacy",
      "id": "batch-storage-and-privacy"
    },
    {
      "level": "h2",
      "text": "FAQ",
      "id": "faq"
    }
  ],
  "url": "llms-txt#stream-results-file-in-memory-efficient-chunks,-processing-one-at-a-time",
  "links": []
}