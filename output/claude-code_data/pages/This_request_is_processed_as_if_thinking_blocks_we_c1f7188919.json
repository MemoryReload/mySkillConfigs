{
  "title": "This request is processed as if thinking blocks were never present",
  "content": "json\n\"cache_control\": {\n    \"type\": \"ephemeral\",\n    \"ttl\": \"5m\" | \"1h\"\n}\njson\n{\n    \"usage\": {\n        \"input_tokens\": ...,\n        \"cache_read_input_tokens\": ...,\n        \"cache_creation_input_tokens\": ...,\n        \"output_tokens\": ...,\n\n\"cache_creation\": {\n            \"ephemeral_5m_input_tokens\": 456,\n            \"ephemeral_1h_input_tokens\": 100,\n        }\n    }\n}\nbash Shell\ncurl https://api.anthropic.com/v1/messages \\\n     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n     --header \"anthropic-version: 2023-06-01\" \\\n     --header \"content-type: application/json\" \\\n     --data \\\n'{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"system\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"You are an AI assistant tasked with analyzing legal documents.\"\n        },\n        {\n            \"type\": \"text\",\n            \"text\": \"Here is the full text of a complex legal agreement: [Insert full text of a 50-page legal agreement here]\",\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ],\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What are the key terms and conditions in this agreement?\"\n        }\n    ]\n}'\npython Python\nimport anthropic\nclient = anthropic.Anthropic()\n\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=[\n        {\n            \"type\": \"text\",\n            \"text\": \"You are an AI assistant tasked with analyzing legal documents.\"\n        },\n        {\n            \"type\": \"text\",\n            \"text\": \"Here is the full text of a complex legal agreement: [Insert full text of a 50-page legal agreement here]\",\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What are the key terms and conditions in this agreement?\"\n        }\n    ]\n)\nprint(response.model_dump_json())\ntypescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic();\n\nconst response = await client.messages.create({\n  model: \"claude-sonnet-4-5\",\n  max_tokens: 1024,\n  system: [\n    {\n        \"type\": \"text\",\n        \"text\": \"You are an AI assistant tasked with analyzing legal documents.\"\n    },\n    {\n        \"type\": \"text\",\n        \"text\": \"Here is the full text of a complex legal agreement: [Insert full text of a 50-page legal agreement here]\",\n        \"cache_control\": {\"type\": \"ephemeral\"}\n    }\n  ],\n  messages: [\n    {\n        \"role\": \"user\",\n        \"content\": \"What are the key terms and conditions in this agreement?\"\n    }\n  ]\n});\nconsole.log(response);\njava Java\nimport java.util.List;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.models.messages.CacheControlEphemeral;\nimport com.anthropic.models.messages.Message;\nimport com.anthropic.models.messages.MessageCreateParams;\nimport com.anthropic.models.messages.Model;\nimport com.anthropic.models.messages.TextBlockParam;\n\npublic class LegalDocumentAnalysisExample {\n\npublic static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\nMessageCreateParams params = MessageCreateParams.builder()\n                .model(Model.CLAUDE_OPUS_4_20250514)\n                .maxTokens(1024)\n                .systemOfTextBlockParams(List.of(\n                        TextBlockParam.builder()\n                                .text(\"You are an AI assistant tasked with analyzing legal documents.\")\n                                .build(),\n                        TextBlockParam.builder()\n                                .text(\"Here is the full text of a complex legal agreement: [Insert full text of a 50-page legal agreement here]\")\n                                .cacheControl(CacheControlEphemeral.builder().build())\n                                .build()\n                ))\n                .addUserMessage(\"What are the key terms and conditions in this agreement?\")\n                .build();\n\nMessage message = client.messages().create(params);\n        System.out.println(message);\n    }\n}\nbash Shell\ncurl https://api.anthropic.com/v1/messages \\\n     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n     --header \"anthropic-version: 2023-06-01\" \\\n     --header \"content-type: application/json\" \\\n     --data \\\n'{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"tools\": [\n        {\n            \"name\": \"get_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"The unit of temperature, either celsius or fahrenheit\"\n                    }\n                },\n                \"required\": [\"location\"]\n            }\n        },\n        # many more tools\n        {\n            \"name\": \"get_time\",\n            \"description\": \"Get the current time in a given time zone\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"timezone\": {\n                        \"type\": \"string\",\n                        \"description\": \"The IANA time zone name, e.g. America/Los_Angeles\"\n                    }\n                },\n                \"required\": [\"timezone\"]\n            },\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ],\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the weather and time in New York?\"\n        }\n    ]\n}'\npython Python\nimport anthropic\nclient = anthropic.Anthropic()\n\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    tools=[\n        {\n            \"name\": \"get_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"The unit of temperature, either 'celsius' or 'fahrenheit'\"\n                    }\n                },\n                \"required\": [\"location\"]\n            },\n        },\n        # many more tools\n        {\n            \"name\": \"get_time\",\n            \"description\": \"Get the current time in a given time zone\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"timezone\": {\n                        \"type\": \"string\",\n                        \"description\": \"The IANA time zone name, e.g. America/Los_Angeles\"\n                    }\n                },\n                \"required\": [\"timezone\"]\n            },\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What's the weather and time in New York?\"\n        }\n    ]\n)\nprint(response.model_dump_json())\ntypescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic();\n\nconst response = await client.messages.create({\n    model: \"claude-sonnet-4-5\",\n    max_tokens: 1024,\n    tools=[\n        {\n            \"name\": \"get_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"],\n                        \"description\": \"The unit of temperature, either 'celsius' or 'fahrenheit'\"\n                    }\n                },\n                \"required\": [\"location\"]\n            },\n        },\n        // many more tools\n        {\n            \"name\": \"get_time\",\n            \"description\": \"Get the current time in a given time zone\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"timezone\": {\n                        \"type\": \"string\",\n                        \"description\": \"The IANA time zone name, e.g. America/Los_Angeles\"\n                    }\n                },\n                \"required\": [\"timezone\"]\n            },\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ],\n    messages: [\n        {\n            \"role\": \"user\",\n            \"content\": \"What's the weather and time in New York?\"\n        }\n    ]\n});\nconsole.log(response);\njava Java\nimport java.util.List;\nimport java.util.Map;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.core.JsonValue;\nimport com.anthropic.models.messages.CacheControlEphemeral;\nimport com.anthropic.models.messages.Message;\nimport com.anthropic.models.messages.MessageCreateParams;\nimport com.anthropic.models.messages.Model;\nimport com.anthropic.models.messages.Tool;\nimport com.anthropic.models.messages.Tool.InputSchema;\n\npublic class ToolsWithCacheControlExample {\n\npublic static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n// Weather tool schema\n        InputSchema weatherSchema = InputSchema.builder()\n                .properties(JsonValue.from(Map.of(\n                        \"location\", Map.of(\n                                \"type\", \"string\",\n                                \"description\", \"The city and state, e.g. San Francisco, CA\"\n                        ),\n                        \"unit\", Map.of(\n                                \"type\", \"string\",\n                                \"enum\", List.of(\"celsius\", \"fahrenheit\"),\n                                \"description\", \"The unit of temperature, either celsius or fahrenheit\"\n                        )\n                )))\n                .putAdditionalProperty(\"required\", JsonValue.from(List.of(\"location\")))\n                .build();\n\n// Time tool schema\n        InputSchema timeSchema = InputSchema.builder()\n                .properties(JsonValue.from(Map.of(\n                        \"timezone\", Map.of(\n                                \"type\", \"string\",\n                                \"description\", \"The IANA time zone name, e.g. America/Los_Angeles\"\n                        )\n                )))\n                .putAdditionalProperty(\"required\", JsonValue.from(List.of(\"timezone\")))\n                .build();\n\nMessageCreateParams params = MessageCreateParams.builder()\n                .model(Model.CLAUDE_OPUS_4_20250514)\n                .maxTokens(1024)\n                .addTool(Tool.builder()\n                        .name(\"get_weather\")\n                        .description(\"Get the current weather in a given location\")\n                        .inputSchema(weatherSchema)\n                        .build())\n                .addTool(Tool.builder()\n                        .name(\"get_time\")\n                        .description(\"Get the current time in a given time zone\")\n                        .inputSchema(timeSchema)\n                        .cacheControl(CacheControlEphemeral.builder().build())\n                        .build())\n                .addUserMessage(\"What is the weather and time in New York?\")\n                .build();\n\nMessage message = client.messages().create(params);\n        System.out.println(message);\n    }\n}\nbash Shell\ncurl https://api.anthropic.com/v1/messages \\\n     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n     --header \"anthropic-version: 2023-06-01\" \\\n     --header \"content-type: application/json\" \\\n     --data \\\n'{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"system\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"...long system prompt\",\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ],\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Hello, can you tell me more about the solar system?\",\n                }\n            ]\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Certainly! The solar system is the collection of celestial bodies that orbit our Sun. It consists of eight planets, numerous moons, asteroids, comets, and other objects. The planets, in order from closest to farthest from the Sun, are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Each planet has its own unique characteristics and features. Is there a specific aspect of the solar system you would like to know more about?\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Good to know.\"\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Tell me more about Mars.\",\n                    \"cache_control\": {\"type\": \"ephemeral\"}\n                }\n            ]\n        }\n    ]\n}'\npython Python\nimport anthropic\nclient = anthropic.Anthropic()\n\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    system=[\n        {\n            \"type\": \"text\",\n            \"text\": \"...long system prompt\",\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ],\n    messages=[\n        # ...long conversation so far\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Hello, can you tell me more about the solar system?\",\n                }\n            ]\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Certainly! The solar system is the collection of celestial bodies that orbit our Sun. It consists of eight planets, numerous moons, asteroids, comets, and other objects. The planets, in order from closest to farthest from the Sun, are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Each planet has its own unique characteristics and features. Is there a specific aspect of the solar system you'd like to know more about?\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Good to know.\"\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Tell me more about Mars.\",\n                    \"cache_control\": {\"type\": \"ephemeral\"}\n                }\n            ]\n        }\n    ]\n)\nprint(response.model_dump_json())\ntypescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic();\n\nconst response = await client.messages.create({\n    model: \"claude-sonnet-4-5\",\n    max_tokens: 1024,\n    system=[\n        {\n            \"type\": \"text\",\n            \"text\": \"...long system prompt\",\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ],\n    messages=[\n        // ...long conversation so far\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Hello, can you tell me more about the solar system?\",\n                }\n            ]\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Certainly! The solar system is the collection of celestial bodies that orbit our Sun. It consists of eight planets, numerous moons, asteroids, comets, and other objects. The planets, in order from closest to farthest from the Sun, are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Each planet has its own unique characteristics and features. Is there a specific aspect of the solar system you'd like to know more about?\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Good to know.\"\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Tell me more about Mars.\",\n                    \"cache_control\": {\"type\": \"ephemeral\"}\n                }\n            ]\n        }\n    ]\n});\nconsole.log(response);\njava Java\nimport java.util.List;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.models.messages.CacheControlEphemeral;\nimport com.anthropic.models.messages.ContentBlockParam;\nimport com.anthropic.models.messages.Message;\nimport com.anthropic.models.messages.MessageCreateParams;\nimport com.anthropic.models.messages.Model;\nimport com.anthropic.models.messages.TextBlockParam;\n\npublic class ConversationWithCacheControlExample {\n\npublic static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n// Create ephemeral system prompt\n        TextBlockParam systemPrompt = TextBlockParam.builder()\n                .text(\"...long system prompt\")\n                .cacheControl(CacheControlEphemeral.builder().build())\n                .build();\n\n// Create message params\n        MessageCreateParams params = MessageCreateParams.builder()\n                .model(Model.CLAUDE_OPUS_4_20250514)\n                .maxTokens(1024)\n                .systemOfTextBlockParams(List.of(systemPrompt))\n                // First user message (without cache control)\n                .addUserMessage(\"Hello, can you tell me more about the solar system?\")\n                // Assistant response\n                .addAssistantMessage(\"Certainly! The solar system is the collection of celestial bodies that orbit our Sun. It consists of eight planets, numerous moons, asteroids, comets, and other objects. The planets, in order from closest to farthest from the Sun, are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. Each planet has its own unique characteristics and features. Is there a specific aspect of the solar system you would like to know more about?\")\n                // Second user message (with cache control)\n                .addUserMessageOfBlockParams(List.of(\n                        ContentBlockParam.ofText(TextBlockParam.builder()\n                                .text(\"Good to know.\")\n                                .build()),\n                        ContentBlockParam.ofText(TextBlockParam.builder()\n                                .text(\"Tell me more about Mars.\")\n                                .cacheControl(CacheControlEphemeral.builder().build())\n                                .build())\n                ))\n                .build();\n\nMessage message = client.messages().create(params);\n        System.out.println(message);\n    }\n}\nbash Shell\ncurl https://api.anthropic.com/v1/messages \\\n     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n     --header \"anthropic-version: 2023-06-01\" \\\n     --header \"content-type: application/json\" \\\n     --data \\\n'{\n    \"model\": \"claude-sonnet-4-5\",\n    \"max_tokens\": 1024,\n    \"tools\": [\n        {\n            \"name\": \"search_documents\",\n            \"description\": \"Search through the knowledge base\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"Search query\"\n                    }\n                },\n                \"required\": [\"query\"]\n            }\n        },\n        {\n            \"name\": \"get_document\",\n            \"description\": \"Retrieve a specific document by ID\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"doc_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"Document ID\"\n                    }\n                },\n                \"required\": [\"doc_id\"]\n            },\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ],\n    \"system\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"You are a helpful research assistant with access to a document knowledge base.\\n\\n# Instructions\\n- Always search for relevant documents before answering\\n- Provide citations for your sources\\n- Be objective and accurate in your responses\\n- If multiple documents contain relevant information, synthesize them\\n- Acknowledge when information is not available in the knowledge base\",\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        },\n        {\n            \"type\": \"text\",\n            \"text\": \"# Knowledge Base Context\\n\\nHere are the relevant documents for this conversation:\\n\\n## Document 1: Solar System Overview\\nThe solar system consists of the Sun and all objects that orbit it...\\n\\n## Document 2: Planetary Characteristics\\nEach planet has unique features. Mercury is the smallest planet...\\n\\n## Document 3: Mars Exploration\\nMars has been a target of exploration for decades...\\n\\n[Additional documents...]\",\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ],\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you search for information about Mars rovers?\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\n                    \"type\": \"tool_use\",\n                    \"id\": \"tool_1\",\n                    \"name\": \"search_documents\",\n                    \"input\": {\"query\": \"Mars rovers\"}\n                }\n            ]\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"tool_result\",\n                    \"tool_use_id\": \"tool_1\",\n                    \"content\": \"Found 3 relevant documents: Document 3 (Mars Exploration), Document 7 (Rover Technology), Document 9 (Mission History)\"\n                }\n            ]\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"I found 3 relevant documents about Mars rovers. Let me get more details from the Mars Exploration document.\"\n                }\n            ]\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Yes, please tell me about the Perseverance rover specifically.\",\n                    \"cache_control\": {\"type\": \"ephemeral\"}\n                }\n            ]\n        }\n    ]\n}'\npython Python\nimport anthropic\nclient = anthropic.Anthropic()\n\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-5\",\n    max_tokens=1024,\n    tools=[\n        {\n            \"name\": \"search_documents\",\n            \"description\": \"Search through the knowledge base\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"Search query\"\n                    }\n                },\n                \"required\": [\"query\"]\n            }\n        },\n        {\n            \"name\": \"get_document\",\n            \"description\": \"Retrieve a specific document by ID\",\n            \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"doc_id\": {\n                        \"type\": \"string\",\n                        \"description\": \"Document ID\"\n                    }\n                },\n                \"required\": [\"doc_id\"]\n            },\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ],\n    system=[\n        {\n            \"type\": \"text\",\n            \"text\": \"You are a helpful research assistant with access to a document knowledge base.\\n\\n# Instructions\\n- Always search for relevant documents before answering\\n- Provide citations for your sources\\n- Be objective and accurate in your responses\\n- If multiple documents contain relevant information, synthesize them\\n- Acknowledge when information is not available in the knowledge base\",\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        },\n        {\n            \"type\": \"text\",\n            \"text\": \"# Knowledge Base Context\\n\\nHere are the relevant documents for this conversation:\\n\\n## Document 1: Solar System Overview\\nThe solar system consists of the Sun and all objects that orbit it...\\n\\n## Document 2: Planetary Characteristics\\nEach planet has unique features. Mercury is the smallest planet...\\n\\n## Document 3: Mars Exploration\\nMars has been a target of exploration for decades...\\n\\n[Additional documents...]\",\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Can you search for information about Mars rovers?\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\n                    \"type\": \"tool_use\",\n                    \"id\": \"tool_1\",\n                    \"name\": \"search_documents\",\n                    \"input\": {\"query\": \"Mars rovers\"}\n                }\n            ]\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"tool_result\",\n                    \"tool_use_id\": \"tool_1\",\n                    \"content\": \"Found 3 relevant documents: Document 3 (Mars Exploration), Document 7 (Rover Technology), Document 9 (Mission History)\"\n                }\n            ]\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"I found 3 relevant documents about Mars rovers. Let me get more details from the Mars Exploration document.\"\n                }\n            ]\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Yes, please tell me about the Perseverance rover specifically.\",\n                    \"cache_control\": {\"type\": \"ephemeral\"}\n                }\n            ]\n        }\n    ]\n)\nprint(response.model_dump_json())\ntypescript TypeScript\nimport Anthropic from '@anthropic-ai/sdk';\n\nconst client = new Anthropic();\n\nconst response = await client.messages.create({\n    model: \"claude-sonnet-4-5\",\n    max_tokens: 1024,\n    tools: [\n        {\n            name: \"search_documents\",\n            description: \"Search through the knowledge base\",\n            input_schema: {\n                type: \"object\",\n                properties: {\n                    query: {\n                        type: \"string\",\n                        description: \"Search query\"\n                    }\n                },\n                required: [\"query\"]\n            }\n        },\n        {\n            name: \"get_document\",\n            description: \"Retrieve a specific document by ID\",\n            input_schema: {\n                type: \"object\",\n                properties: {\n                    doc_id: {\n                        type: \"string\",\n                        description: \"Document ID\"\n                    }\n                },\n                required: [\"doc_id\"]\n            },\n            cache_control: { type: \"ephemeral\" }\n        }\n    ],\n    system: [\n        {\n            type: \"text\",\n            text: \"You are a helpful research assistant with access to a document knowledge base.\\n\\n# Instructions\\n- Always search for relevant documents before answering\\n- Provide citations for your sources\\n- Be objective and accurate in your responses\\n- If multiple documents contain relevant information, synthesize them\\n- Acknowledge when information is not available in the knowledge base\",\n            cache_control: { type: \"ephemeral\" }\n        },\n        {\n            type: \"text\",\n            text: \"# Knowledge Base Context\\n\\nHere are the relevant documents for this conversation:\\n\\n## Document 1: Solar System Overview\\nThe solar system consists of the Sun and all objects that orbit it...\\n\\n## Document 2: Planetary Characteristics\\nEach planet has unique features. Mercury is the smallest planet...\\n\\n## Document 3: Mars Exploration\\nMars has been a target of exploration for decades...\\n\\n[Additional documents...]\",\n            cache_control: { type: \"ephemeral\" }\n        }\n    ],\n    messages: [\n        {\n            role: \"user\",\n            content: \"Can you search for information about Mars rovers?\"\n        },\n        {\n            role: \"assistant\",\n            content: [\n                {\n                    type: \"tool_use\",\n                    id: \"tool_1\",\n                    name: \"search_documents\",\n                    input: { query: \"Mars rovers\" }\n                }\n            ]\n        },\n        {\n            role: \"user\",\n            content: [\n                {\n                    type: \"tool_result\",\n                    tool_use_id: \"tool_1\",\n                    content: \"Found 3 relevant documents: Document 3 (Mars Exploration), Document 7 (Rover Technology), Document 9 (Mission History)\"\n                }\n            ]\n        },\n        {\n            role: \"assistant\",\n            content: [\n                {\n                    type: \"text\",\n                    text: \"I found 3 relevant documents about Mars rovers. Let me get more details from the Mars Exploration document.\"\n                }\n            ]\n        },\n        {\n            role: \"user\",\n            content: [\n                {\n                    type: \"text\",\n                    text: \"Yes, please tell me about the Perseverance rover specifically.\",\n                    cache_control: { type: \"ephemeral\" }\n                }\n            ]\n        }\n    ]\n});\nconsole.log(response);\njava Java\nimport java.util.List;\nimport java.util.Map;\n\nimport com.anthropic.client.AnthropicClient;\nimport com.anthropic.client.okhttp.AnthropicOkHttpClient;\nimport com.anthropic.core.JsonValue;\nimport com.anthropic.models.messages.CacheControlEphemeral;\nimport com.anthropic.models.messages.ContentBlockParam;\nimport com.anthropic.models.messages.Message;\nimport com.anthropic.models.messages.MessageCreateParams;\nimport com.anthropic.models.messages.Model;\nimport com.anthropic.models.messages.TextBlockParam;\nimport com.anthropic.models.messages.Tool;\nimport com.anthropic.models.messages.Tool.InputSchema;\nimport com.anthropic.models.messages.ToolResultBlockParam;\nimport com.anthropic.models.messages.ToolUseBlockParam;\n\npublic class MultipleCacheBreakpointsExample {\n\npublic static void main(String[] args) {\n        AnthropicClient client = AnthropicOkHttpClient.fromEnv();\n\n// Search tool schema\n        InputSchema searchSchema = InputSchema.builder()\n                .properties(JsonValue.from(Map.of(\n                        \"query\", Map.of(\n                                \"type\", \"string\",\n                                \"description\", \"Search query\"\n                        )\n                )))\n                .putAdditionalProperty(\"required\", JsonValue.from(List.of(\"query\")))\n                .build();\n\n// Get document tool schema\n        InputSchema getDocSchema = InputSchema.builder()\n                .properties(JsonValue.from(Map.of(\n                        \"doc_id\", Map.of(\n                                \"type\", \"string\",\n                                \"description\", \"Document ID\"\n                        )\n                )))\n                .putAdditionalProperty(\"required\", JsonValue.from(List.of(\"doc_id\")))\n                .build();\n\nMessageCreateParams params = MessageCreateParams.builder()\n                .model(Model.CLAUDE_OPUS_4_20250514)\n                .maxTokens(1024)\n                // Tools with cache control on the last one\n                .addTool(Tool.builder()\n                        .name(\"search_documents\")\n                        .description(\"Search through the knowledge base\")\n                        .inputSchema(searchSchema)\n                        .build())\n                .addTool(Tool.builder()\n                        .name(\"get_document\")\n                        .description(\"Retrieve a specific document by ID\")\n                        .inputSchema(getDocSchema)\n                        .cacheControl(CacheControlEphemeral.builder().build())\n                        .build())\n                // System prompts with cache control on instructions and context separately\n                .systemOfTextBlockParams(List.of(\n                        TextBlockParam.builder()\n                                .text(\"You are a helpful research assistant with access to a document knowledge base.\\n\\n# Instructions\\n- Always search for relevant documents before answering\\n- Provide citations for your sources\\n- Be objective and accurate in your responses\\n- If multiple documents contain relevant information, synthesize them\\n- Acknowledge when information is not available in the knowledge base\")\n                                .cacheControl(CacheControlEphemeral.builder().build())\n                                .build(),\n                        TextBlockParam.builder()\n                                .text(\"# Knowledge Base Context\\n\\nHere are the relevant documents for this conversation:\\n\\n## Document 1: Solar System Overview\\nThe solar system consists of the Sun and all objects that orbit it...\\n\\n## Document 2: Planetary Characteristics\\nEach planet has unique features. Mercury is the smallest planet...\\n\\n## Document 3: Mars Exploration\\nMars has been a target of exploration for decades...\\n\\n[Additional documents...]\")\n                                .cacheControl(CacheControlEphemeral.builder().build())\n                                .build()\n                ))\n                // Conversation history\n                .addUserMessage(\"Can you search for information about Mars rovers?\")\n                .addAssistantMessageOfBlockParams(List.of(\n                        ContentBlockParam.ofToolUse(ToolUseBlockParam.builder()\n                                .id(\"tool_1\")\n                                .name(\"search_documents\")\n                                .input(JsonValue.from(Map.of(\"query\", \"Mars rovers\")))\n                                .build())\n                ))\n                .addUserMessageOfBlockParams(List.of(\n                        ContentBlockParam.ofToolResult(ToolResultBlockParam.builder()\n                                .toolUseId(\"tool_1\")\n                                .content(\"Found 3 relevant documents: Document 3 (Mars Exploration), Document 7 (Rover Technology), Document 9 (Mission History)\")\n                                .build())\n                ))\n                .addAssistantMessageOfBlockParams(List.of(\n                        ContentBlockParam.ofText(TextBlockParam.builder()\n                                .text(\"I found 3 relevant documents about Mars rovers. Let me get more details from the Mars Exploration document.\")\n                                .build())\n                ))\n                .addUserMessageOfBlockParams(List.of(\n                        ContentBlockParam.ofText(TextBlockParam.builder()\n                                .text(\"Yes, please tell me about the Perseverance rover specifically.\")\n                                .cacheControl(CacheControlEphemeral.builder().build())\n                                .build())\n                ))\n                .build();\n\nMessage message = client.messages().create(params);\n        System.out.println(message);\n    }\n}\n\ntotal_input_tokens = cache_read_input_tokens + cache_creation_input_tokens + input_tokens\n    python Python\n      python client.beta.prompt_caching.messages.create(...)\n      python Python\n      python client.messages.create(...)\n      typescript TypeScript\n      client.beta.promptCaching.messages.create(...)\n      typescript\n      client.messages.create(...)\n      ```\n  \n</section>",
  "code_samples": [
    {
      "code": "When a non-tool-result user block is included, it designates a new assistant loop and all previous thinking blocks are removed from context.\n\nFor more detailed information, see the [extended thinking documentation](/docs/en/build-with-claude/extended-thinking#understanding-thinking-block-caching-behavior).\n\n---\n## Cache storage and sharing\n\n- **Organization Isolation**: Caches are isolated between organizations. Different organizations never share caches, even if they use identical prompts.\n\n- **Exact Matching**: Cache hits require 100% identical prompt segments, including all text and images up to and including the block marked with cache control.\n\n- **Output Token Generation**: Prompt caching has no effect on output token generation. The response you receive will be identical to what you would get if prompt caching was not used.\n\n---\n## 1-hour cache duration\n\nIf you find that 5 minutes is too short, Anthropic also offers a 1-hour cache duration [at additional cost](#pricing).\n\nTo use the extended cache, include `ttl` in the `cache_control` definition like this:",
      "language": "unknown"
    },
    {
      "code": "The response will include detailed cache information like the following:",
      "language": "unknown"
    },
    {
      "code": "Note that the current `cache_creation_input_tokens` field equals the sum of the values in the `cache_creation` object.\n\n### When to use the 1-hour cache\n\nIf you have prompts that are used at a regular cadence (i.e., system prompts that are used more frequently than every 5 minutes), continue to use the 5-minute cache, since this will continue to be refreshed at no additional charge.\n\nThe 1-hour cache is best used in the following scenarios:\n- When you have prompts that are likely used less frequently than 5 minutes, but more frequently than every hour. For example, when an agentic side-agent will take longer than 5 minutes, or when storing a long chat conversation with a user and you generally expect that user may not respond in the next 5 minutes.\n- When latency is important and your follow up prompts may be sent beyond 5 minutes.\n- When you want to improve your rate limit utilization, since cache hits are not deducted against your rate limit.\n\n<Note>\nThe 5-minute and 1-hour cache behave the same with respect to latency. You will generally see improved time-to-first-token for long documents.\n</Note>\n\n### Mixing different TTLs\n\nYou can use both 1-hour and 5-minute cache controls in the same request, but with an important constraint: Cache entries with longer TTL must appear before shorter TTLs (i.e., a 1-hour cache entry must appear before any 5-minute cache entries).\n\nWhen mixing TTLs, we determine three billing locations in your prompt:\n1. Position `A`: The token count at the highest cache hit (or 0 if no hits).\n2. Position `B`: The token count at the highest 1-hour `cache_control` block after `A` (or equals `A` if none exist).\n3. Position `C`: The token count at the last `cache_control` block.\n\n<Note>\nIf `B` and/or `C` are larger than `A`, they will necessarily be cache misses, because `A` is the highest cache hit.\n</Note>\n\nYou'll be charged for:\n1. Cache read tokens for `A`.\n2. 1-hour cache write tokens for `(B - A)`.\n3. 5-minute cache write tokens for `(C - B)`.\n\nHere are 3 examples. This depicts the input tokens of 3 requests, each of which has different cache hits and cache misses. Each has a different calculated pricing, shown in the colored boxes, as a result.\n![Mixing TTLs Diagram](/docs/images/prompt-cache-mixed-ttl.svg)\n\n---\n## Prompt caching examples\n\nTo help you get started with prompt caching, we've prepared a [prompt caching cookbook](https://platform.claude.com/cookbook/misc-prompt-caching) with detailed examples and best practices.\n\nBelow, we've included several code snippets that showcase various prompt caching patterns. These examples demonstrate how to implement caching in different scenarios, helping you understand the practical applications of this feature:\n\n<section title=\"Large context caching example\">\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\nThis example demonstrates basic prompt caching usage, caching the full text of the legal agreement as a prefix while keeping the user instruction uncached.\n\nFor the first request:\n- `input_tokens`: Number of tokens in the user message only\n- `cache_creation_input_tokens`: Number of tokens in the entire system message, including the legal document\n- `cache_read_input_tokens`: 0 (no cache hit on first request)\n\nFor subsequent requests within the cache lifetime:\n- `input_tokens`: Number of tokens in the user message only\n- `cache_creation_input_tokens`: 0 (no new cache creation)\n- `cache_read_input_tokens`: Number of tokens in the entire cached system message\n\n</section>\n<section title=\"Caching tool definitions\">\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nIn this example, we demonstrate caching tool definitions.\n\nThe `cache_control` parameter is placed on the final tool (`get_time`) to designate all of the tools as part of the static prefix.\n\nThis means that all tool definitions, including `get_weather` and any other tools defined before `get_time`, will be cached as a single prefix.\n\nThis approach is useful when you have a consistent set of tools that you want to reuse across multiple requests without re-processing them each time.\n\nFor the first request:\n- `input_tokens`: Number of tokens in the user message\n- `cache_creation_input_tokens`: Number of tokens in all tool definitions and system prompt\n- `cache_read_input_tokens`: 0 (no cache hit on first request)\n\nFor subsequent requests within the cache lifetime:\n- `input_tokens`: Number of tokens in the user message\n- `cache_creation_input_tokens`: 0 (no new cache creation)\n- `cache_read_input_tokens`: Number of tokens in all cached tool definitions and system prompt\n\n</section>\n\n<section title=\"Continuing a multi-turn conversation\">\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nIn this example, we demonstrate how to use prompt caching in a multi-turn conversation.\n\nDuring each turn, we mark the final block of the final message with `cache_control` so the conversation can be incrementally cached. The system will automatically lookup and use the longest previously cached sequence of blocks for follow-up messages. That is, blocks that were previously marked with a `cache_control` block are later not marked with this, but they will still be considered a cache hit (and also a cache refresh!) if they are hit within 5 minutes.\n\nIn addition, note that the `cache_control` parameter is placed on the system message. This is to ensure that if this gets evicted from the cache (after not being used for more than 5 minutes), it will get added back to the cache on the next request.\n\nThis approach is useful for maintaining context in ongoing conversations without repeatedly processing the same information.\n\nWhen this is set up properly, you should see the following in the usage response of each request:\n- `input_tokens`: Number of tokens in the new user message (will be minimal)\n- `cache_creation_input_tokens`: Number of tokens in the new assistant and user turns\n- `cache_read_input_tokens`: Number of tokens in the conversation up to the previous turn\n\n</section>\n\n<section title=\"Putting it all together: Multiple cache breakpoints\">\n\n<CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n\nThis comprehensive example demonstrates how to use all 4 available cache breakpoints to optimize different parts of your prompt:\n\n1. **Tools cache** (cache breakpoint 1): The `cache_control` parameter on the last tool definition caches all tool definitions.\n\n2. **Reusable instructions cache** (cache breakpoint 2): The static instructions in the system prompt are cached separately. These instructions rarely change between requests.\n\n3. **RAG context cache** (cache breakpoint 3): The knowledge base documents are cached independently, allowing you to update the RAG documents without invalidating the tools or instructions cache.\n\n4. **Conversation history cache** (cache breakpoint 4): The assistant's response is marked with `cache_control` to enable incremental caching of the conversation as it progresses.\n\nThis approach provides maximum flexibility:\n- If you only update the final user message, all four cache segments are reused\n- If you update the RAG documents but keep the same tools and instructions, the first two cache segments are reused\n- If you change the conversation but keep the same tools, instructions, and documents, the first three segments are reused\n- Each cache breakpoint can be invalidated independently based on what changes in your application\n\nFor the first request:\n- `input_tokens`: Tokens in the final user message\n- `cache_creation_input_tokens`: Tokens in all cached segments (tools + instructions + RAG documents + conversation history)\n- `cache_read_input_tokens`: 0 (no cache hits)\n\nFor subsequent requests with only a new user message:\n- `input_tokens`: Tokens in the new user message only\n- `cache_creation_input_tokens`: Any new tokens added to conversation history\n- `cache_read_input_tokens`: All previously cached tokens (tools + instructions + RAG documents + previous conversation)\n\nThis pattern is especially powerful for:\n- RAG applications with large document contexts\n- Agent systems that use multiple tools\n- Long-running conversations that need to maintain context\n- Applications that need to optimize different parts of the prompt independently\n\n</section>\n\n---\n## FAQ\n\n  <section title=\"Do I need multiple cache breakpoints or is one at the end sufficient?\">\n\n    **In most cases, a single cache breakpoint at the end of your static content is sufficient.** The system automatically checks for cache hits at all previous content block boundaries (up to 20 blocks before your breakpoint) and uses the longest matching sequence of cached blocks.\n\n    You only need multiple breakpoints if:\n    - You have more than 20 content blocks before your desired cache point\n    - You want to cache sections that update at different frequencies independently\n    - You need explicit control over what gets cached for cost optimization\n\n    Example: If you have system instructions (rarely change) and RAG context (changes daily), you might use two breakpoints to cache them separately.\n  \n</section>\n\n  <section title=\"Do cache breakpoints add extra cost?\">\n\n    No, cache breakpoints themselves are free. You only pay for:\n    - Writing content to cache (25% more than base input tokens for 5-minute TTL)\n    - Reading from cache (10% of base input token price)\n    - Regular input tokens for uncached content\n\n    The number of breakpoints doesn't affect pricing - only the amount of content cached and read matters.\n  \n</section>\n\n  <section title=\"How do I calculate total input tokens from the usage fields?\">\n\n    The usage response includes three separate input token fields that together represent your total input:",
      "language": "unknown"
    },
    {
      "code": "- `cache_read_input_tokens`: Tokens retrieved from cache (everything before cache breakpoints that was cached)\n    - `cache_creation_input_tokens`: New tokens being written to cache (at cache breakpoints)\n    - `input_tokens`: Tokens **after the last cache breakpoint** that aren't cached\n\n    **Important:** `input_tokens` does NOT represent all input tokens - only the portion after your last cache breakpoint. If you have cached content, `input_tokens` will typically be much smaller than your total input.\n\n    **Example:** With a 200K token document cached and a 50 token user question:\n    - `cache_read_input_tokens`: 200,000\n    - `cache_creation_input_tokens`: 0\n    - `input_tokens`: 50\n    - **Total**: 200,050 tokens\n\n    This breakdown is critical for understanding both your costs and rate limit usage. See [Tracking cache performance](#tracking-cache-performance) for more details.\n  \n</section>\n\n  <section title=\"What is the cache lifetime?\">\n\n    The cache's default minimum lifetime (TTL) is 5 minutes. This lifetime is refreshed each time the cached content is used.\n\n    If you find that 5 minutes is too short, Anthropic also offers a [1-hour cache TTL](#1-hour-cache-duration).\n  \n</section>\n\n  <section title=\"How many cache breakpoints can I use?\">\n\n    You can define up to 4 cache breakpoints (using `cache_control` parameters) in your prompt.\n  \n</section>\n\n  <section title=\"Is prompt caching available for all models?\">\n\n    No, prompt caching is currently only available for Claude Opus 4.5, Claude Opus 4.1, Claude Opus 4, Claude Sonnet 4.5, Claude Sonnet 4, Claude Sonnet 3.7 ([deprecated](/docs/en/about-claude/model-deprecations)), Claude Haiku 4.5, Claude Haiku 3.5 ([deprecated](/docs/en/about-claude/model-deprecations)), and Claude Haiku 3.\n  \n</section>\n\n  <section title=\"How does prompt caching work with extended thinking?\">\n\n    Cached system prompts and tools will be reused when thinking parameters change. However, thinking changes (enabling/disabling or budget changes) will invalidate previously cached prompt prefixes with messages content.\n\n    For more details on cache invalidation, see [What invalidates the cache](#what-invalidates-the-cache).\n\n    For more on extended thinking, including its interaction with tool use and prompt caching, see the [extended thinking documentation](/docs/en/build-with-claude/extended-thinking#extended-thinking-and-prompt-caching).\n  \n</section>\n\n  <section title=\"How do I enable prompt caching?\">\n\n    To enable prompt caching, include at least one `cache_control` breakpoint in your API request.\n  \n</section>\n\n  <section title=\"Can I use prompt caching with other API features?\">\n\n    Yes, prompt caching can be used alongside other API features like tool use and vision capabilities. However, changing whether there are images in a prompt or modifying tool use settings will break the cache.\n\n    For more details on cache invalidation, see [What invalidates the cache](#what-invalidates-the-cache).\n  \n</section>\n\n  <section title=\"How does prompt caching affect pricing?\">\n\n    Prompt caching introduces a new pricing structure where cache writes cost 25% more than base input tokens, while cache hits cost only 10% of the base input token price.\n  \n</section>\n\n  <section title=\"Can I manually clear the cache?\">\n\n    Currently, there's no way to manually clear the cache. Cached prefixes automatically expire after a minimum of 5 minutes of inactivity.\n  \n</section>\n\n  <section title=\"How can I track the effectiveness of my caching strategy?\">\n\n    You can monitor cache performance using the `cache_creation_input_tokens` and `cache_read_input_tokens` fields in the API response.\n  \n</section>\n\n  <section title=\"What can break the cache?\">\n\n    See [What invalidates the cache](#what-invalidates-the-cache) for more details on cache invalidation, including a list of changes that require creating a new cache entry.\n  \n</section>\n\n  <section title=\"How does prompt caching handle privacy and data separation?\">\n\nPrompt caching is designed with strong privacy and data separation measures:\n\n1. Cache keys are generated using a cryptographic hash of the prompts up to the cache control point. This means only requests with identical prompts can access a specific cache.\n\n2. Caches are organization-specific. Users within the same organization can access the same cache if they use identical prompts, but caches are not shared across different organizations, even for identical prompts.\n\n3. The caching mechanism is designed to maintain the integrity and privacy of each unique conversation or context.\n\n4. It's safe to use `cache_control` anywhere in your prompts. For cost efficiency, it's better to exclude highly variable parts (e.g., user's arbitrary input) from caching.\n\nThese measures ensure that prompt caching maintains data privacy and security while offering performance benefits.\n  \n</section>\n  <section title=\"Can I use prompt caching with the Batches API?\">\n\n    Yes, it is possible to use prompt caching with your [Batches API](/docs/en/build-with-claude/batch-processing) requests. However, because asynchronous batch requests can be processed concurrently and in any order, cache hits are provided on a best-effort basis.\n\n    The [1-hour cache](#1-hour-cache-duration) can help improve your cache hits. The most cost effective way of using it is the following:\n    - Gather a set of message requests that have a shared prefix.\n    - Send a batch request with just a single request that has this shared prefix and a 1-hour cache block. This will get written to the 1-hour cache.\n    - As soon as this is complete, submit the rest of the requests. You will have to monitor the job to know when it completes.\n\n    This is typically better than using the 5-minute cache simply because its common for batch requests to take between 5 minutes and 1 hour to complete. Were considering ways to improve these cache hit rates and making this process more straightforward.\n  \n</section>\n  <section title=\"Why am I seeing the error `AttributeError: 'Beta' object has no attribute 'prompt_caching'` in Python?\">\n\n  This error typically appears when you have upgraded your SDK or you are using outdated code examples. Prompt caching is now generally available, so you no longer need the beta prefix. Instead of:\n    <CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n    Simply use:\n    <CodeGroup>",
      "language": "unknown"
    },
    {
      "code": "</CodeGroup>\n  \n</section>\n  <section title=\"Why am I seeing 'TypeError: Cannot read properties of undefined (reading 'messages')'?\">\n\n  This error typically appears when you have upgraded your SDK or you are using outdated code examples. Prompt caching is now generally available, so you no longer need the beta prefix. Instead of:",
      "language": "unknown"
    },
    {
      "code": "Simply use:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "Cache storage and sharing",
      "id": "cache-storage-and-sharing"
    },
    {
      "level": "h2",
      "text": "1-hour cache duration",
      "id": "1-hour-cache-duration"
    },
    {
      "level": "h3",
      "text": "When to use the 1-hour cache",
      "id": "when-to-use-the-1-hour-cache"
    },
    {
      "level": "h3",
      "text": "Mixing different TTLs",
      "id": "mixing-different-ttls"
    },
    {
      "level": "h2",
      "text": "Prompt caching examples",
      "id": "prompt-caching-examples"
    },
    {
      "level": "h2",
      "text": "FAQ",
      "id": "faq"
    }
  ],
  "url": "llms-txt#this-request-is-processed-as-if-thinking-blocks-were-never-present",
  "links": []
}